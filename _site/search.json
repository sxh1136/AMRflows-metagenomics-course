[
  {
    "objectID": "10. Read vs Assembly based analyses.html",
    "href": "10. Read vs Assembly based analyses.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "There are two main approaches to analyse metagenomic data - read based and assembly based. In most studies, reasearchers will use a combination of both approaches to provide a more comprehensive understanding of the metagenome.\n\n\nRead-based analysis is a popular and straightforward method for metagenomic data analysis, especially when dealing with complex environmental samples. It involves the direct examination seqeuencing reads which offers several advantages:\n\nRead-based analysis is generally faster since it doesn’t require the computationally intensive assembly step.\nIt can detect rare microbial species, which may be missed in assembly-based approaches.\nIt is computationally less demanding and suitable for modest computing resources.\n\n\n\n\n\n\nReads are mapped to reference databases using tools such as MetaPhlAn, Kraken, or NCBI’s BLAST to identify taxonomic affliations of each read.\nAbundance of different taxa are calculated based on the number of reads which align to the taxa’s sequences. This quanitification is done automatically by tools like MetaPhlAn.\n\n\n\n\n\nReads can also be mapped to functional databases such as KEGG, COG, or Pfam, to infer their metabolic function. Abundance can be calculated in a similar fashion to taxonomy. Tools such as HUMAnN can functionally profile and quantiy abundance of microbial pathways automatically.\n\n\n\n\n\nTools such as DESeq2, edgeR, and LEfSe use statistical tests to identify taxonomic or functional differences between samples.\n\n\n\n\n\n\nRead-based methods are limited by the read length, which can hinder the ability to accurately assign taxonomic or functional annotations to complex genomes or genes.\nRead-based approaches rely on reference databases, potentially missing novel species or functions not present in the databases.\nEstimating abundance from read counts can be challenging due to variations in genome sizes and biases in sequencing.\n\n\n\n\n\nAssembly-based analysis involves reconstructing longer genomic sequences (contigs) from metagenomic reads to obtain a more comprehensive view of the microbial community. This approach is particularly valuable when studying novel or less-characterized environments. Here are the key advantages:\n\nIt can provide much longer sequences and in some cases near-complete genomes of individual species within the community.\nLong contigs allow for the prediction of functional genes and pathways with higher confidence.\nIt can uncover novel microbial species or functions not present in reference databases.\n\n\n\n\n\nAs the name implies reads must be assembled (stitched) into contigs first. In metagenomics we use de novo assemblers, as we do not have prior knowledge of the composition of the input DNA. Two of the most commonly used metagenomic assemblers are metaSPAdes and MEGAHIT. Both of these are de Brujin graph based assemblers. What that entails exactly is far beyond the scope of this course but if you want to read more about that here’s a great paper on the topic: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5531759/ (Beware: there’s lots of maths).\n\n\n\n\nContigs can be further grouped into clusters based on similarites in their sequence composition and coverage profiles. These bins are often referred to as metagenome-assembled-genomes (MAGs).\nMAGs should then be assessed for completeness and contamination with a tool such as CheckM.\n\n\n\n\n\nTools such as Kraken or GTDB-TK can be used to taxonomically classify contigs.\nThe original reads can be mapped to the contigs to estimate their abundance.\n\n\n\n\n\nCoding sequences are first predicted by tools like Prodigal or MetaGeneMark.\nThe coding sequences are then annotated using the same databases used in read-based analysis.\nTools like Prokka, combine the two steps above for fast functional annotation of metagenomes.\n\n\n\n\n\nBy mapping reads to the contig set, the same differential analysis performed in read-based analyses can be used (DESeq2, edgeR, and LefSe). This can help pinpoint microbial species or functions that play crucial roles in specific environments.\n\n\n\n\n\n\nAssembly-based approaches demand substantial computational resources and time, especially for large datasets.\nComplex metagenomes may result in fragmented assemblies, limiting the ability to capture complete genomes.\nChimeric contigs, formed from multiple species, can sometimes be produced by assemblers leading to misinterpretations.\nNot all reads will be assembled into contigs, especially those belonging to low abundance species.\n\n\n\n\n\nThe choice between read-based and assembly-based analysis depends on the goals of your study, the complexity of the metagenomic sample, and the available computational resources.\n\nRead-based analysis is typically used when you want to find the bulk taxonomic and/or functional compoisistion of your metagenome(s), and compare these compositions between samples/sites.\nAssembly-based analysis is useful for when you want to find the functional potential of certain taxa in your samples, or for phylogenic analyses, or when you are interested in variants.\n\nIn practice, many researchers use a combination of both approaches to balance their benefits and limitations. In previous studies, I have utilised read-based analyses for taxonomic classification of my samples, and then used assembly-based analysis for identification of novel species, and characterising the functional potential of certain MAGs.\nIn the next sections, we’ll put what we have learnt about metagenomic analyses into practice."
  },
  {
    "objectID": "10. Read vs Assembly based analyses.html#read-vs.-assembly-based-analysis-of-metagenomic-data",
    "href": "10. Read vs Assembly based analyses.html#read-vs.-assembly-based-analysis-of-metagenomic-data",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "There are two main approaches to analyse metagenomic data - read based and assembly based. In most studies, reasearchers will use a combination of both approaches to provide a more comprehensive understanding of the metagenome.\n\n\nRead-based analysis is a popular and straightforward method for metagenomic data analysis, especially when dealing with complex environmental samples. It involves the direct examination seqeuencing reads which offers several advantages:\n\nRead-based analysis is generally faster since it doesn’t require the computationally intensive assembly step.\nIt can detect rare microbial species, which may be missed in assembly-based approaches.\nIt is computationally less demanding and suitable for modest computing resources.\n\n\n\n\n\n\nReads are mapped to reference databases using tools such as MetaPhlAn, Kraken, or NCBI’s BLAST to identify taxonomic affliations of each read.\nAbundance of different taxa are calculated based on the number of reads which align to the taxa’s sequences. This quanitification is done automatically by tools like MetaPhlAn.\n\n\n\n\n\nReads can also be mapped to functional databases such as KEGG, COG, or Pfam, to infer their metabolic function. Abundance can be calculated in a similar fashion to taxonomy. Tools such as HUMAnN can functionally profile and quantiy abundance of microbial pathways automatically.\n\n\n\n\n\nTools such as DESeq2, edgeR, and LEfSe use statistical tests to identify taxonomic or functional differences between samples.\n\n\n\n\n\n\nRead-based methods are limited by the read length, which can hinder the ability to accurately assign taxonomic or functional annotations to complex genomes or genes.\nRead-based approaches rely on reference databases, potentially missing novel species or functions not present in the databases.\nEstimating abundance from read counts can be challenging due to variations in genome sizes and biases in sequencing.\n\n\n\n\n\nAssembly-based analysis involves reconstructing longer genomic sequences (contigs) from metagenomic reads to obtain a more comprehensive view of the microbial community. This approach is particularly valuable when studying novel or less-characterized environments. Here are the key advantages:\n\nIt can provide much longer sequences and in some cases near-complete genomes of individual species within the community.\nLong contigs allow for the prediction of functional genes and pathways with higher confidence.\nIt can uncover novel microbial species or functions not present in reference databases.\n\n\n\n\n\nAs the name implies reads must be assembled (stitched) into contigs first. In metagenomics we use de novo assemblers, as we do not have prior knowledge of the composition of the input DNA. Two of the most commonly used metagenomic assemblers are metaSPAdes and MEGAHIT. Both of these are de Brujin graph based assemblers. What that entails exactly is far beyond the scope of this course but if you want to read more about that here’s a great paper on the topic: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5531759/ (Beware: there’s lots of maths).\n\n\n\n\nContigs can be further grouped into clusters based on similarites in their sequence composition and coverage profiles. These bins are often referred to as metagenome-assembled-genomes (MAGs).\nMAGs should then be assessed for completeness and contamination with a tool such as CheckM.\n\n\n\n\n\nTools such as Kraken or GTDB-TK can be used to taxonomically classify contigs.\nThe original reads can be mapped to the contigs to estimate their abundance.\n\n\n\n\n\nCoding sequences are first predicted by tools like Prodigal or MetaGeneMark.\nThe coding sequences are then annotated using the same databases used in read-based analysis.\nTools like Prokka, combine the two steps above for fast functional annotation of metagenomes.\n\n\n\n\n\nBy mapping reads to the contig set, the same differential analysis performed in read-based analyses can be used (DESeq2, edgeR, and LefSe). This can help pinpoint microbial species or functions that play crucial roles in specific environments.\n\n\n\n\n\n\nAssembly-based approaches demand substantial computational resources and time, especially for large datasets.\nComplex metagenomes may result in fragmented assemblies, limiting the ability to capture complete genomes.\nChimeric contigs, formed from multiple species, can sometimes be produced by assemblers leading to misinterpretations.\nNot all reads will be assembled into contigs, especially those belonging to low abundance species.\n\n\n\n\n\nThe choice between read-based and assembly-based analysis depends on the goals of your study, the complexity of the metagenomic sample, and the available computational resources.\n\nRead-based analysis is typically used when you want to find the bulk taxonomic and/or functional compoisistion of your metagenome(s), and compare these compositions between samples/sites.\nAssembly-based analysis is useful for when you want to find the functional potential of certain taxa in your samples, or for phylogenic analyses, or when you are interested in variants.\n\nIn practice, many researchers use a combination of both approaches to balance their benefits and limitations. In previous studies, I have utilised read-based analyses for taxonomic classification of my samples, and then used assembly-based analysis for identification of novel species, and characterising the functional potential of certain MAGs.\nIn the next sections, we’ll put what we have learnt about metagenomic analyses into practice."
  },
  {
    "objectID": "11. Read-based-taxonomic-classification.html",
    "href": "11. Read-based-taxonomic-classification.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In this section, we will dive into the practical aspects of read-based taxonomic classification using MetaPhlAn, a popular tool for profiling the taxonomic composition of metagenomic samples. MetaPhlAn stands for “Metagenomic Phylogenetic Analysis” and is widely used for its accuracy and efficiency in characterizing microbial communities from shotgun metagenomic sequencing data.\n\n\nMetaPhlAn is a tool developed by the Segata Lab for taxonomic profiling of metagenomic samples. It uses unique clade-specific marker genes to identify and quantify the presence of microbes in a given metagenomic dataset. The output of running MetaPhlAn is a relative abundance table which can be visualised in various ways. You can (should) read more about it on it’s github page: https://github.com/biobakery/MetaPhlAn.\n\n\nWe can use conda to install the latest version of MetaPhlAn as we learnt before. Detailled instructions on installing the tool can be found at https://github.com/biobakery/MetaPhlAn/wiki/MetaPhlAn-4#installation.\nconda create --name metaphlan python=3.7\nconda activate metaphlan\nconda install -c bioconda metaphlan\nWe will also need to download MetaPhlAn’s required databases. This can be done with:\nmetaphlan --install --bowtie2db &lt;database folder&gt;\nReplace  with the path to where you want to download the database. You need a minimum of 15 GB of space.\n\n\n\n\nTo perform taxonomic classification with MetaPhlAn, you’ll need to provide it with metagenomic sequencing data in FASTQ format.\nFor single-end reads:\nmetaphlan input.fastq --input_type fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 8 -o profiled_metagenome.txt \nFor paired-end reads:\nmetaphlan input_1.fastq,input_2.fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 8 --input_type fastq -o profiled_metagenome.txt\n\nThe flag --bowtie2out saves the intermediate alignment files into metagenome.bowtie2.bz2. This allows us to rerun metaphlan extremley quickly.\nThe flag --nproc specifies how many CPUs we allocate to metaphlan. Generally, more CPUs the faster it will run but this depends on your computer’s specifications.\nMetaphlan can also estimate the unknown fraction of the metagenome using the percentage of reads mapping to their database to scale the relative abundance profiles. This can be enabled with the --unclassified_estimation flag.\n\nThe output of metaphlan is two column tab seperated text file. The first column contains the different microbial clades that have been predicted whilst the second column contains the corresponding relative abundances (%). If we look inside profiled_metagenome.txt this is what we would see:\n# Database version\n# Metaphlan command executed\n# Number of reads processed \n# SampleID Metaphlan_Analysis\n# clade_name NCBI_tax_id relative_abundance additional_species\nUNCLASSIFIED 5.7891\nk__Bacteria 2  84.6758\nk__Bacteria|p__Actinobacteria 2|201174 45.8922\nk__Bacteria|p__Proteobacteria 2|1224 23.1078\nk__Bacteria|p__Actinobacteria|c__Actinobacteria 2|201174|1760 44.6724\nk__Bacteria|p__Proteobacteria|c__Betaproteobacteria 2|1224|28216 22.0178\n...\nThe first five rows which begin with # are header rows which give more information about your metaphlan run. The last header row contains four headers for each of the columns below it: * clade_name - The taxonomy of the clade reported on this row. Each taxonomic level is indicated by a prefix: Kingdom - k__, Phylym - p, Classs - c, Order - o, Family - f, Genus - g, s - species, species genome bin (SGB) - t__. * NCBI_tax_id - The corresponding NCBI taxon ID for the clade in clade_name. * relative_abundnace - The clade’s relative abundance in %. Clades are hierarchally summed meaning each taxonomic level will equal 100%. Note that if unclassifed estimation is performed, each taxonomic level will add up to 100% minus unclassified percentage. * additional_species - In cases where a clade is represented by multiple species, alternative species names are listed in this column. clade_name still lists the representative species.\nLets use grep to find out more about our family of interest - Enterococcaceae.\ngrep \"Enterococcaceae\" profiled_metagenome | head -1\nNote: We use the pipe | symbol for the first time here. This command redirects the output of the command before it to the command after it allowing us to chain commands together. In the example above the grepped text is fed into the head command which returns us just the first line of the grep output.\nThis yields:\nk__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Enterococcaceae      2|1239|91061|186826|81852       14.517849033370496\nThis output shows that info about the family Enterococcaceae in our metagenome was successfully extracted, with it making up 14.5% of its composition.\n\n\n\nIn most studies we will be investigating the taxonomic profile of multiple metagenomic samples. Therefore it is useful to combine multiple metaphlan outputs into a matrix for comparison and visualisation. Fortunately, metaphlan includes an utility script for this exact purpose:\nmerge_metaphlan_tables.py *.txt &gt; merged_abundance_table.txt\nThis merged table only retains sample names, clades names, and relative abundances. You can easily view this file as a spreadsheet using Excel or with less.\n# -S is the less flag for chopping off the ends of lines when they are longer than the screen width instead of wrapping. I find that this often makings files like merged metaphlan files much easier to read.\nless -S merged_abundance_table.txt\nThe first few lines would look like this:\n#mpa_vJan21_CHOCOPhlAnSGB_202103\n\nclade_name Sample_1 Sample_2 Sample_3 Sample_4\n\nUNCLASSIFIED 10.2 9.43 2.98 4.44 \nk__Bacteria 88.2 90.4 80.04 87.99\nk__Bacteria|p__Bacteroidetes 75.78146 2.342 23.1323 10.41\nk__Bacteria|p__Firmicutes 11.84254 3.12346 43.1642 69.96495\nk__Bacteria|p__Bacteroidetes|c__Bacteroidia 73.78146 2.2123 20.09994 10.41\n\n\n\nThe most common and widely applicable method for visualising taxonomic profiles such as those produced by metaphlan is with a heatmap.\nThe first step of making a heatmap is to decide what taxonomic level you want to visualise at. This will depend on the goal of your study and what you want to display in your figure. For this example we will use the species level.\nTo extract only species level clades from our merged table we use the following command:\ngrep -E \"clade|s__\" merged_abundance_table.txt | grep -Ev \"t__\" | sed \"s/^.*s__//g\" &gt; merged_abundance_table_species.txt\nThis command first uses grep to extract the header line by matching “clade” and all rows with species assignments. The second grep removes lines which have been resolved to SGB level leaving us with only species level clades. The sed command removes everything before and including “s__” leaving us with only the species name in the clade_name column (You can read more about the sed command here). The &gt; redirects the output of the chain of commands to the file merged_abundance_table_species.txt instead of just printing it out (You can read more about redirect outputs here).\nThe first few lines of the new file looks like this:\nclade_name Sample_1 Sample_2 Sample_3 Sample_4\n\nEnterococcus_faecium 22.09729 1.13 0.0 3.334 \nParabacteroides_distasonis 0.0 1.52429 11.4621 0.0 \nBacteroides_fragilis 16.21854 2.8948 0.0 0.0 \nMethanobrevibacter_smithii 0.0 0.0 19.34642 0.0 \n\n\n\nThis data is now ready for visualisation. There are many different ways to produce a heatmap, especially if you have experience with data focused programming languages like python or R. Most methods are very comparable and only differ in aesthetics. In this tutorial we’ll be using hclust2 which can be installed via Conda.\n*I’ve been informed that hclust2 might not play nicely with ARM based hardware such as Apple’s M series Macs. If you do encounter issues with this and have some experience with R programming, a good alternative tool is pheatmap. A good guide on hierarchical clustering and creating heatmaps with pheatmap is linked here. If you’re just following the code, you can start at the “Removing Non-Numeric Labels” section, as our metaphlan output is already in matrix form.\nconda install -c biobakery hclust2\nCheck if the tool has been installed correctly and have a look at the available flags with hclust2 --help.\nTo invoke the hclust2 python script and create a heatmap based on the species level table we generated earlier we can run the folliwng command:\nhclust2.py \\\n-i merged_abundance_table_species.txt \\\n-o metaphlan_heatmap_species.png \\\n--f_dist_f braycurtis \\\n--s_dist_f braycurtis \\\n--minv 0.1 \\\n--ftop 50 \\\n--cell_aspect_ratio 1 \\\n--flabel_size 10 |\n--slabel_size 10 \\\n--max_flabel_len 100 |\n--max_slabel_len 100 \\\n--dpi 300\nThis script takes our table as input with the -i flag and outputs a heatmap png file as detailed with the -o flag. For our distance functions we have selected Bray-Curtis dissimilarity for both our features (--f_dist_f), which in this case are our species and our samples (--s_dist_f). Bray-Curtis is a commonly used index which is used to assess how similar two samples are. It is particularly useful because it can handle the presence of zeros i.e. samples containing a relative abundance of 0 for certain species. A Bray-Curtis dissimilarity of 0 indicates that two samples have exactly the same composition, and a value of 1 indicates that their composition does not overlap at all. We then use the flags --minv and --ftop to set the minimum abundance value to be display and to only show the top 50 species in the heatmap. The rest of the flags affect the aestehtics of the heatmap. Can you work out what they affect?\nHere’s an example of a heatmap created this way from a real published paper:\n\n\n\nHeat map and hierarchical clustering of the top 40 taxa mapped by MetaPhlAn2 from Highlander et al. 2023\n\n\nCan you identify what the most abundant taxa in this heatmap is?\nCan you identify any clustering of samples?\nDo you notice any differences between the data used to create the heatmap here and the data we prepared ourselves?"
  },
  {
    "objectID": "11. Read-based-taxonomic-classification.html#taxonomic-classification-with-metaphlan",
    "href": "11. Read-based-taxonomic-classification.html#taxonomic-classification-with-metaphlan",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In this section, we will dive into the practical aspects of read-based taxonomic classification using MetaPhlAn, a popular tool for profiling the taxonomic composition of metagenomic samples. MetaPhlAn stands for “Metagenomic Phylogenetic Analysis” and is widely used for its accuracy and efficiency in characterizing microbial communities from shotgun metagenomic sequencing data.\n\n\nMetaPhlAn is a tool developed by the Segata Lab for taxonomic profiling of metagenomic samples. It uses unique clade-specific marker genes to identify and quantify the presence of microbes in a given metagenomic dataset. The output of running MetaPhlAn is a relative abundance table which can be visualised in various ways. You can (should) read more about it on it’s github page: https://github.com/biobakery/MetaPhlAn.\n\n\nWe can use conda to install the latest version of MetaPhlAn as we learnt before. Detailled instructions on installing the tool can be found at https://github.com/biobakery/MetaPhlAn/wiki/MetaPhlAn-4#installation.\nconda create --name metaphlan python=3.7\nconda activate metaphlan\nconda install -c bioconda metaphlan\nWe will also need to download MetaPhlAn’s required databases. This can be done with:\nmetaphlan --install --bowtie2db &lt;database folder&gt;\nReplace  with the path to where you want to download the database. You need a minimum of 15 GB of space.\n\n\n\n\nTo perform taxonomic classification with MetaPhlAn, you’ll need to provide it with metagenomic sequencing data in FASTQ format.\nFor single-end reads:\nmetaphlan input.fastq --input_type fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 8 -o profiled_metagenome.txt \nFor paired-end reads:\nmetaphlan input_1.fastq,input_2.fastq --bowtie2out metagenome.bowtie2.bz2 --nproc 8 --input_type fastq -o profiled_metagenome.txt\n\nThe flag --bowtie2out saves the intermediate alignment files into metagenome.bowtie2.bz2. This allows us to rerun metaphlan extremley quickly.\nThe flag --nproc specifies how many CPUs we allocate to metaphlan. Generally, more CPUs the faster it will run but this depends on your computer’s specifications.\nMetaphlan can also estimate the unknown fraction of the metagenome using the percentage of reads mapping to their database to scale the relative abundance profiles. This can be enabled with the --unclassified_estimation flag.\n\nThe output of metaphlan is two column tab seperated text file. The first column contains the different microbial clades that have been predicted whilst the second column contains the corresponding relative abundances (%). If we look inside profiled_metagenome.txt this is what we would see:\n# Database version\n# Metaphlan command executed\n# Number of reads processed \n# SampleID Metaphlan_Analysis\n# clade_name NCBI_tax_id relative_abundance additional_species\nUNCLASSIFIED 5.7891\nk__Bacteria 2  84.6758\nk__Bacteria|p__Actinobacteria 2|201174 45.8922\nk__Bacteria|p__Proteobacteria 2|1224 23.1078\nk__Bacteria|p__Actinobacteria|c__Actinobacteria 2|201174|1760 44.6724\nk__Bacteria|p__Proteobacteria|c__Betaproteobacteria 2|1224|28216 22.0178\n...\nThe first five rows which begin with # are header rows which give more information about your metaphlan run. The last header row contains four headers for each of the columns below it: * clade_name - The taxonomy of the clade reported on this row. Each taxonomic level is indicated by a prefix: Kingdom - k__, Phylym - p, Classs - c, Order - o, Family - f, Genus - g, s - species, species genome bin (SGB) - t__. * NCBI_tax_id - The corresponding NCBI taxon ID for the clade in clade_name. * relative_abundnace - The clade’s relative abundance in %. Clades are hierarchally summed meaning each taxonomic level will equal 100%. Note that if unclassifed estimation is performed, each taxonomic level will add up to 100% minus unclassified percentage. * additional_species - In cases where a clade is represented by multiple species, alternative species names are listed in this column. clade_name still lists the representative species.\nLets use grep to find out more about our family of interest - Enterococcaceae.\ngrep \"Enterococcaceae\" profiled_metagenome | head -1\nNote: We use the pipe | symbol for the first time here. This command redirects the output of the command before it to the command after it allowing us to chain commands together. In the example above the grepped text is fed into the head command which returns us just the first line of the grep output.\nThis yields:\nk__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Enterococcaceae      2|1239|91061|186826|81852       14.517849033370496\nThis output shows that info about the family Enterococcaceae in our metagenome was successfully extracted, with it making up 14.5% of its composition.\n\n\n\nIn most studies we will be investigating the taxonomic profile of multiple metagenomic samples. Therefore it is useful to combine multiple metaphlan outputs into a matrix for comparison and visualisation. Fortunately, metaphlan includes an utility script for this exact purpose:\nmerge_metaphlan_tables.py *.txt &gt; merged_abundance_table.txt\nThis merged table only retains sample names, clades names, and relative abundances. You can easily view this file as a spreadsheet using Excel or with less.\n# -S is the less flag for chopping off the ends of lines when they are longer than the screen width instead of wrapping. I find that this often makings files like merged metaphlan files much easier to read.\nless -S merged_abundance_table.txt\nThe first few lines would look like this:\n#mpa_vJan21_CHOCOPhlAnSGB_202103\n\nclade_name Sample_1 Sample_2 Sample_3 Sample_4\n\nUNCLASSIFIED 10.2 9.43 2.98 4.44 \nk__Bacteria 88.2 90.4 80.04 87.99\nk__Bacteria|p__Bacteroidetes 75.78146 2.342 23.1323 10.41\nk__Bacteria|p__Firmicutes 11.84254 3.12346 43.1642 69.96495\nk__Bacteria|p__Bacteroidetes|c__Bacteroidia 73.78146 2.2123 20.09994 10.41\n\n\n\nThe most common and widely applicable method for visualising taxonomic profiles such as those produced by metaphlan is with a heatmap.\nThe first step of making a heatmap is to decide what taxonomic level you want to visualise at. This will depend on the goal of your study and what you want to display in your figure. For this example we will use the species level.\nTo extract only species level clades from our merged table we use the following command:\ngrep -E \"clade|s__\" merged_abundance_table.txt | grep -Ev \"t__\" | sed \"s/^.*s__//g\" &gt; merged_abundance_table_species.txt\nThis command first uses grep to extract the header line by matching “clade” and all rows with species assignments. The second grep removes lines which have been resolved to SGB level leaving us with only species level clades. The sed command removes everything before and including “s__” leaving us with only the species name in the clade_name column (You can read more about the sed command here). The &gt; redirects the output of the chain of commands to the file merged_abundance_table_species.txt instead of just printing it out (You can read more about redirect outputs here).\nThe first few lines of the new file looks like this:\nclade_name Sample_1 Sample_2 Sample_3 Sample_4\n\nEnterococcus_faecium 22.09729 1.13 0.0 3.334 \nParabacteroides_distasonis 0.0 1.52429 11.4621 0.0 \nBacteroides_fragilis 16.21854 2.8948 0.0 0.0 \nMethanobrevibacter_smithii 0.0 0.0 19.34642 0.0 \n\n\n\nThis data is now ready for visualisation. There are many different ways to produce a heatmap, especially if you have experience with data focused programming languages like python or R. Most methods are very comparable and only differ in aesthetics. In this tutorial we’ll be using hclust2 which can be installed via Conda.\n*I’ve been informed that hclust2 might not play nicely with ARM based hardware such as Apple’s M series Macs. If you do encounter issues with this and have some experience with R programming, a good alternative tool is pheatmap. A good guide on hierarchical clustering and creating heatmaps with pheatmap is linked here. If you’re just following the code, you can start at the “Removing Non-Numeric Labels” section, as our metaphlan output is already in matrix form.\nconda install -c biobakery hclust2\nCheck if the tool has been installed correctly and have a look at the available flags with hclust2 --help.\nTo invoke the hclust2 python script and create a heatmap based on the species level table we generated earlier we can run the folliwng command:\nhclust2.py \\\n-i merged_abundance_table_species.txt \\\n-o metaphlan_heatmap_species.png \\\n--f_dist_f braycurtis \\\n--s_dist_f braycurtis \\\n--minv 0.1 \\\n--ftop 50 \\\n--cell_aspect_ratio 1 \\\n--flabel_size 10 |\n--slabel_size 10 \\\n--max_flabel_len 100 |\n--max_slabel_len 100 \\\n--dpi 300\nThis script takes our table as input with the -i flag and outputs a heatmap png file as detailed with the -o flag. For our distance functions we have selected Bray-Curtis dissimilarity for both our features (--f_dist_f), which in this case are our species and our samples (--s_dist_f). Bray-Curtis is a commonly used index which is used to assess how similar two samples are. It is particularly useful because it can handle the presence of zeros i.e. samples containing a relative abundance of 0 for certain species. A Bray-Curtis dissimilarity of 0 indicates that two samples have exactly the same composition, and a value of 1 indicates that their composition does not overlap at all. We then use the flags --minv and --ftop to set the minimum abundance value to be display and to only show the top 50 species in the heatmap. The rest of the flags affect the aestehtics of the heatmap. Can you work out what they affect?\nHere’s an example of a heatmap created this way from a real published paper:\n\n\n\nHeat map and hierarchical clustering of the top 40 taxa mapped by MetaPhlAn2 from Highlander et al. 2023\n\n\nCan you identify what the most abundant taxa in this heatmap is?\nCan you identify any clustering of samples?\nDo you notice any differences between the data used to create the heatmap here and the data we prepared ourselves?"
  },
  {
    "objectID": "2. Intro-to-metagenomics.html",
    "href": "2. Intro-to-metagenomics.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Metagenomics is the study of genetic material directly from the an environmental or clinical sample by sequencing. This is in contrast to microbial isolate sequencing which relies on a culture of identical cells a source of genetic material.\nMetagenomics is all about understanding the sum of the genetic material within a sample. It has been used for a wide range of applications including:\n\nAgriculture and soil microbiome\nScreening for novel enzymes, antibiotics, and other useful compounds\nFunctional ecology\nEnvironmental pollution and remediation\nWildlife conservation\nGut microbiome\nHealthcare diagnostics\n\n\n\nYou may sometimes see metagenomics as an umbrella term for two sequencing strategies: amplicon sequencing, and metagenomic sequencing. Amplicon sequencing, which is also know as metabarcoding, targets one or more specific genes via primers, and amplifies these genes for sequencing. The most commonly used gene is the 16S rRNA gene. This approach is mainly used to identify taxonomic profiles. Metagenomics on the other hand, targets entire genomes, and allows for the detection of encoded proteins and pathways, as well as taxonomic assignments at species or even strain resolution. For this reason, amplicon sequencing should not be referred to as metagenomics.\n\n\n\nAmplicon vs Metagenomic Sequencing. Figure replicated from Happy Belly Bioinformatics\n\n\n\n\n\nIn this course, we will be focusing on metagenomic sequencing (aka shotgun sequencing). In metagenomics, DNA is extracted from the sample of interest and sheared into small fragments which are then sequenced. This results in DNA sequences known as reads, which correspond to sections across all the different genomes within the sample. Some of these reads will be sampled from taxonomically informative regions such as marker genes (e.g. 16S rRNA, 18S rRNA, ITS) or regions that encode biological functions (e.g. antibiotic resistance genes, efflux, nucleotide salvage). These reads allow us to simultaneously identify what is in the sample, and what they able to do.\n\n\n\nThese days metagenomic sequencing is typically carried out on an Illumina* sequencing platform, although sometimes you will see samples that are sequenced using other high-throughput sequencing technologies such as 454 pyrosequencing, Ion semiconductor sequencing, and SOLiD seqeuncing. Long-read metagenomic sequencing with Nanopore or PacBio technologies is a new but rapidly evolving field which produces substantially longer reads. In this course we will only be looking at Illumina sequencing reads, as this is by far the most common source of metagenomic data, but some of the workflow steps are applicable to the output of the other platforms.\n\n\n\nThe metagenomic sequencing process starts with purified DNA. This DNA extraction and purification process will differ based on the sample of interest and goals of the experiment, but most protocols use silica spin column kits as they can rapidly extract DNA with high purity. Once DNA is purified, a sequencing library needs to be generated. An Illumina sequencing library can be created either with tagmentation or sonication. In tagmentation, transposases are used to simultaneously cut the DNA into small fragments (50 - 500 bp) and add adaptor sequences. Sonication is used to mechanically fragment DNA sequences into similar sizes, before adaptors are added using DNA polymerase and ligase. These adaptors are made up of three sections: a sequence that is complementary to oligonucleotides attached to the surface of a flow cell, which act as a solid support, a binding site for the sequencing primer, and a barcode sequence. As multiple samples are often sequenced at the same time, the barcode sequence is used an identifier for the sample so that the reads can be grouped together during data analysis.\nThe modified DNA is then washed over the flow cell and the adaptors attach to the complementary solid support. A phase called cluster generation then begins, where hundreds of identical strands of DNA are created through bridge amplication PCR. If you want to read more about this process, Apollo Institute have a great article on it here.\nSequencing primers and fluorescently tagged nucleotides are then added to the flow cell. The primers attach to the binding site on the adaptor sequences, and a DNA polymerase adds a nucleotide to the nascent DNA strand starting from the primer. Each of the four nucleotide bases has a different fluorophore, and after each round of synthesis, they are excited using a light source, resulting in the release of a unique wavelength signal which is recorded by a camera. Only once nucleotide is added in each round by the polymerase, as the fluorophore acts a blocking group. This process repeats until the entire DNA molecule is sequenced. Using this “sequencing by synthesis” approach millions of DNA reads can be sequenced per run.\nIllumina produced a video going over their “sequencing by synthesis” workflow which provides a great 3D schematic representation of the technology:\n\nThis process is quite complex so don’t be concerned if you don’t fully understand it straight away, as it is not essential to the bioinformatics you’ll learn in this course. However, knowledge of the overall process is useful as it allows us to understand where the data is coming from, as well as its benefits and limitations."
  },
  {
    "objectID": "2. Intro-to-metagenomics.html#introduction-to-metagenomics",
    "href": "2. Intro-to-metagenomics.html#introduction-to-metagenomics",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Metagenomics is the study of genetic material directly from the an environmental or clinical sample by sequencing. This is in contrast to microbial isolate sequencing which relies on a culture of identical cells a source of genetic material.\nMetagenomics is all about understanding the sum of the genetic material within a sample. It has been used for a wide range of applications including:\n\nAgriculture and soil microbiome\nScreening for novel enzymes, antibiotics, and other useful compounds\nFunctional ecology\nEnvironmental pollution and remediation\nWildlife conservation\nGut microbiome\nHealthcare diagnostics\n\n\n\nYou may sometimes see metagenomics as an umbrella term for two sequencing strategies: amplicon sequencing, and metagenomic sequencing. Amplicon sequencing, which is also know as metabarcoding, targets one or more specific genes via primers, and amplifies these genes for sequencing. The most commonly used gene is the 16S rRNA gene. This approach is mainly used to identify taxonomic profiles. Metagenomics on the other hand, targets entire genomes, and allows for the detection of encoded proteins and pathways, as well as taxonomic assignments at species or even strain resolution. For this reason, amplicon sequencing should not be referred to as metagenomics.\n\n\n\nAmplicon vs Metagenomic Sequencing. Figure replicated from Happy Belly Bioinformatics\n\n\n\n\n\nIn this course, we will be focusing on metagenomic sequencing (aka shotgun sequencing). In metagenomics, DNA is extracted from the sample of interest and sheared into small fragments which are then sequenced. This results in DNA sequences known as reads, which correspond to sections across all the different genomes within the sample. Some of these reads will be sampled from taxonomically informative regions such as marker genes (e.g. 16S rRNA, 18S rRNA, ITS) or regions that encode biological functions (e.g. antibiotic resistance genes, efflux, nucleotide salvage). These reads allow us to simultaneously identify what is in the sample, and what they able to do.\n\n\n\nThese days metagenomic sequencing is typically carried out on an Illumina* sequencing platform, although sometimes you will see samples that are sequenced using other high-throughput sequencing technologies such as 454 pyrosequencing, Ion semiconductor sequencing, and SOLiD seqeuncing. Long-read metagenomic sequencing with Nanopore or PacBio technologies is a new but rapidly evolving field which produces substantially longer reads. In this course we will only be looking at Illumina sequencing reads, as this is by far the most common source of metagenomic data, but some of the workflow steps are applicable to the output of the other platforms.\n\n\n\nThe metagenomic sequencing process starts with purified DNA. This DNA extraction and purification process will differ based on the sample of interest and goals of the experiment, but most protocols use silica spin column kits as they can rapidly extract DNA with high purity. Once DNA is purified, a sequencing library needs to be generated. An Illumina sequencing library can be created either with tagmentation or sonication. In tagmentation, transposases are used to simultaneously cut the DNA into small fragments (50 - 500 bp) and add adaptor sequences. Sonication is used to mechanically fragment DNA sequences into similar sizes, before adaptors are added using DNA polymerase and ligase. These adaptors are made up of three sections: a sequence that is complementary to oligonucleotides attached to the surface of a flow cell, which act as a solid support, a binding site for the sequencing primer, and a barcode sequence. As multiple samples are often sequenced at the same time, the barcode sequence is used an identifier for the sample so that the reads can be grouped together during data analysis.\nThe modified DNA is then washed over the flow cell and the adaptors attach to the complementary solid support. A phase called cluster generation then begins, where hundreds of identical strands of DNA are created through bridge amplication PCR. If you want to read more about this process, Apollo Institute have a great article on it here.\nSequencing primers and fluorescently tagged nucleotides are then added to the flow cell. The primers attach to the binding site on the adaptor sequences, and a DNA polymerase adds a nucleotide to the nascent DNA strand starting from the primer. Each of the four nucleotide bases has a different fluorophore, and after each round of synthesis, they are excited using a light source, resulting in the release of a unique wavelength signal which is recorded by a camera. Only once nucleotide is added in each round by the polymerase, as the fluorophore acts a blocking group. This process repeats until the entire DNA molecule is sequenced. Using this “sequencing by synthesis” approach millions of DNA reads can be sequenced per run.\nIllumina produced a video going over their “sequencing by synthesis” workflow which provides a great 3D schematic representation of the technology:\n\nThis process is quite complex so don’t be concerned if you don’t fully understand it straight away, as it is not essential to the bioinformatics you’ll learn in this course. However, knowledge of the overall process is useful as it allows us to understand where the data is coming from, as well as its benefits and limitations."
  },
  {
    "objectID": "3. Intro-to-the-command-line.html",
    "href": "3. Intro-to-the-command-line.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In the world of metagenomics, data analysis is a critical component of unraveling complex microbial communities. To use bioinformatics tools and perform analyses, researchers often rely on a fundamental tool known as the command line. The command line, sometimes referred to as the terminal or shell, is a text-based interface that allows users to interact with a computer’s operating system and execute a wide range of tasks, including data manipulation, file management, and running specialized bioinformatics software.\n\n\nUnlike the graphical user interfaces (GUIs) most of us are familiar with, where we interact with programs using windows, icons, and menus, the command line operates purely through text commands. Users type specific commands, often in the form of text strings, into a terminal window, and the computer responds with text-based output. This text-based interface may seem intimidating at first, especially to those new to bioinformatics, but it offers significant advantages for metagenomic analysis.\n\n\n\nThe command line environment is facilitated by software programs called shells. A shell is essentially a command interpreter that acts as an intermediary between the user and the computer’s operating system. It takes your text-based commands, translates them into instructions the computer can understand, and then executes those instructions.\nOne of the most widely used shells in the world of bioinformatics and beyond is Bash (short for “Bourne Again Shell”). Bash is known for its power, flexibility, and extensive support for scripting, making it a favorite among researchers and data analysts. Throughout this course, we’ll primarily use the Bash shell to introduce you to the command line.\n\n\n\n\nEfficiency: The command line allows for precise and efficient control over your computer. You can perform complex tasks quickly by executing a series of commands in a script or by using one-liners, which can be especially beneficial when handling large metagenomic datasets.\nReproducibility: Scripts and command sequences can be saved and shared, ensuring that your analyses are reproducible by you and others. This is crucial in scientific research, as it promotes transparency and the verification of results.\nAccess to Powerful Tools: Many bioinformatics tools and software packages are designed to be used via the command line. These tools offer advanced capabilities for processing, aligning, and analyzing metagenomic data that may not be easily accessible through graphical interfaces.\nRemote Computing: In metagenomics, you often deal with substantial datasets that require significant computational resources. Command line access to remote servers or high-performance computing clusters allows you to analyze data without overloading your local machine.\nCustomization: Command line interfaces offer a high degree of customization. Users can create scripts and workflows tailored to their specific research needs, enabling flexibility in metagenomic analysis.\n\n\n\n\nFor those new to the command line, it may seem like learning a new language. However, it is a skill that can be acquired with practice. In this course, we will introduce you to the command line, with a focus on the Bash shell, and guide you through its basic commands and metagenomics-specific tools. However, I recommend go through one of the many free introduction to command line courses, as they’re be able to cover things more comprehensively. You should be able to complete this course without going through one of these external courses, but we will be going though the basics quite quickly so you may find it difficult if you are new to bioinformatics.\nThe course that I used to learn the command line was “Learn the command line” by Codeacademy. Codeacademy’s courses include an actual command line interface that you can interact with and test commands in. Unfortuately, it is not a free course anymore, but you can sign up for a 7 day free trial. Completing the first three modules should be more than enough to get familiar with the command line.\nCode Academy Course\nAnother course that I’ve heard good things about is the Udemy “Learn The Linux Command Line: Basic Commands” course. Best of all, its free!\nUdemy Course\nIf you complete either of these courses, you can probably skip the sections “Accessing the command line”, “Exploring the shell environment”, and “For loops”!\nIn the next section, we’ll start with the essentials: how to open a terminal, navigate the file system, and begin using Bash commands. So, let’s begin our journey into the world of metagenomic data analysis using the command line and the powerful Bash shell!"
  },
  {
    "objectID": "3. Intro-to-the-command-line.html#what-is-the-command-line",
    "href": "3. Intro-to-the-command-line.html#what-is-the-command-line",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In the world of metagenomics, data analysis is a critical component of unraveling complex microbial communities. To use bioinformatics tools and perform analyses, researchers often rely on a fundamental tool known as the command line. The command line, sometimes referred to as the terminal or shell, is a text-based interface that allows users to interact with a computer’s operating system and execute a wide range of tasks, including data manipulation, file management, and running specialized bioinformatics software.\n\n\nUnlike the graphical user interfaces (GUIs) most of us are familiar with, where we interact with programs using windows, icons, and menus, the command line operates purely through text commands. Users type specific commands, often in the form of text strings, into a terminal window, and the computer responds with text-based output. This text-based interface may seem intimidating at first, especially to those new to bioinformatics, but it offers significant advantages for metagenomic analysis.\n\n\n\nThe command line environment is facilitated by software programs called shells. A shell is essentially a command interpreter that acts as an intermediary between the user and the computer’s operating system. It takes your text-based commands, translates them into instructions the computer can understand, and then executes those instructions.\nOne of the most widely used shells in the world of bioinformatics and beyond is Bash (short for “Bourne Again Shell”). Bash is known for its power, flexibility, and extensive support for scripting, making it a favorite among researchers and data analysts. Throughout this course, we’ll primarily use the Bash shell to introduce you to the command line.\n\n\n\n\nEfficiency: The command line allows for precise and efficient control over your computer. You can perform complex tasks quickly by executing a series of commands in a script or by using one-liners, which can be especially beneficial when handling large metagenomic datasets.\nReproducibility: Scripts and command sequences can be saved and shared, ensuring that your analyses are reproducible by you and others. This is crucial in scientific research, as it promotes transparency and the verification of results.\nAccess to Powerful Tools: Many bioinformatics tools and software packages are designed to be used via the command line. These tools offer advanced capabilities for processing, aligning, and analyzing metagenomic data that may not be easily accessible through graphical interfaces.\nRemote Computing: In metagenomics, you often deal with substantial datasets that require significant computational resources. Command line access to remote servers or high-performance computing clusters allows you to analyze data without overloading your local machine.\nCustomization: Command line interfaces offer a high degree of customization. Users can create scripts and workflows tailored to their specific research needs, enabling flexibility in metagenomic analysis.\n\n\n\n\nFor those new to the command line, it may seem like learning a new language. However, it is a skill that can be acquired with practice. In this course, we will introduce you to the command line, with a focus on the Bash shell, and guide you through its basic commands and metagenomics-specific tools. However, I recommend go through one of the many free introduction to command line courses, as they’re be able to cover things more comprehensively. You should be able to complete this course without going through one of these external courses, but we will be going though the basics quite quickly so you may find it difficult if you are new to bioinformatics.\nThe course that I used to learn the command line was “Learn the command line” by Codeacademy. Codeacademy’s courses include an actual command line interface that you can interact with and test commands in. Unfortuately, it is not a free course anymore, but you can sign up for a 7 day free trial. Completing the first three modules should be more than enough to get familiar with the command line.\nCode Academy Course\nAnother course that I’ve heard good things about is the Udemy “Learn The Linux Command Line: Basic Commands” course. Best of all, its free!\nUdemy Course\nIf you complete either of these courses, you can probably skip the sections “Accessing the command line”, “Exploring the shell environment”, and “For loops”!\nIn the next section, we’ll start with the essentials: how to open a terminal, navigate the file system, and begin using Bash commands. So, let’s begin our journey into the world of metagenomic data analysis using the command line and the powerful Bash shell!"
  },
  {
    "objectID": "4. Accessing the command line.html",
    "href": "4. Accessing the command line.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Before we dive into metagenomic analysis using the command line, it’s important to know how to access this powerful tool. The method you use to access the command line can vary depending on your operating system. The vast majority of bioinfomatics software is written for Unix systems (e.g. Linux, MacOS). However, we can emulate a Unix-like shell for Windows. Alternatively, many researchers connect to servers to carry out their analysis as everyone’s computer, operating system, and available processing power is different. Servers are simply computers that are remotely accessed. Your university may have their own dedicated servers or you may be able to hire one. Here, we’ll cover the most common methods to access the command line for Windows, macOS, and Linux users. Using these, you’ll be able to carry out bioifnormatic analyses directly or connect to external servers. For the latter, I recommend reaching out to the administrators of your server for more information on how to connect.\n\n\nGit Bash\nThis is my preferred windows terminal experience, and you can download and use Git Bash from the official Git website. Git Bash combines the power of the Bash shell with Git commands, making it a versatile choice for command line tasks. There are alternatives such as Windows Terminal but I have limited experience with them.\n\n\n\nTerminal\nTerminal is included in Mac installations by default. You can open it by: * Press Command + Spacebar to open Spotlight Search. * Type Terminal and press Enter: This will open the Terminal application.\n\nYou can also right click the desktop and open “Terminal”.\n\n\n\n\nTerminal\nTerminal is included in Linux distributions by default. * Most Linux distributions have a keyboard shortcut to open the terminal. Common shortcuts include Ctrl + Alt + T or Windows key + T.\n\nAlternatively, right click desktop, and open “Terminal”"
  },
  {
    "objectID": "4. Accessing the command line.html#accessing-the-command-line",
    "href": "4. Accessing the command line.html#accessing-the-command-line",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Before we dive into metagenomic analysis using the command line, it’s important to know how to access this powerful tool. The method you use to access the command line can vary depending on your operating system. The vast majority of bioinfomatics software is written for Unix systems (e.g. Linux, MacOS). However, we can emulate a Unix-like shell for Windows. Alternatively, many researchers connect to servers to carry out their analysis as everyone’s computer, operating system, and available processing power is different. Servers are simply computers that are remotely accessed. Your university may have their own dedicated servers or you may be able to hire one. Here, we’ll cover the most common methods to access the command line for Windows, macOS, and Linux users. Using these, you’ll be able to carry out bioifnormatic analyses directly or connect to external servers. For the latter, I recommend reaching out to the administrators of your server for more information on how to connect.\n\n\nGit Bash\nThis is my preferred windows terminal experience, and you can download and use Git Bash from the official Git website. Git Bash combines the power of the Bash shell with Git commands, making it a versatile choice for command line tasks. There are alternatives such as Windows Terminal but I have limited experience with them.\n\n\n\nTerminal\nTerminal is included in Mac installations by default. You can open it by: * Press Command + Spacebar to open Spotlight Search. * Type Terminal and press Enter: This will open the Terminal application.\n\nYou can also right click the desktop and open “Terminal”.\n\n\n\n\nTerminal\nTerminal is included in Linux distributions by default. * Most Linux distributions have a keyboard shortcut to open the terminal. Common shortcuts include Ctrl + Alt + T or Windows key + T.\n\nAlternatively, right click desktop, and open “Terminal”"
  },
  {
    "objectID": "4. Accessing the command line.html#introduction-to-bash",
    "href": "4. Accessing the command line.html#introduction-to-bash",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "Introduction to Bash",
    "text": "Introduction to Bash\nAs mentioned in the previous section, we will be using the Bash shell throughout this course for our command line interactions. Bash is the default shell for most Linux distributions and is also available for Windows via Git Bash. MacOS uses Z shell which is built on Bash and should behave almost identically.\n\nWhy Bash?\n\nCompatibility: Bash is widely supported across different operating systems, making it a good choice for cross-platform work.\nScripting: Bash is a powerful scripting language, allowing you to automate tasks and create reusable scripts for your metagenomic analyses.\nDocumentation: Bash has a large and active user community, which means you can easily find help and resources online.\n\n\n\nLet’s Test the Waters\nLet’s make sure you’re all set to start this adventure. We’ll do a quick test to make sure your Command Line is up and running.\nOpen your Command Line or Terminal using the method that suits your computer (Command Prompt, Git Bash, Terminal, or equivalent).\nNow, in the window that just opened, type this and press enter:\necho \"Hello, Command Line!\"\nDid you see a friendly “Hello, Command Line!” response? If yes, great job! You’re ready to roll.\n\n\nNavigating the File System\nBefore we start using advanced commands, let’s get comfortable with navigating the file system using the command line. In your terminal, you can use commands like ls (list files), cd (change directory), and pwd (print working directory) to explore your computer’s file structure.\n\nNote that what we typically call folders are called directories on the command line.\n\nExercise\n\nType ls and press Enter. This will show you what’s in your current location (directory).\nNow, type cd followed by a folder name (e.g., cd Documents) and press Enter. You’ve now moved into that folder.\nTo find out where you are now, type pwd and press Enter. It’s like checking your GPS location.\n\nNow that you know how to access the command line and navigate the file system, you’re ready to start using the Bash shell for metagenomic analysis. In the next section, we’ll explore some basic Bash commands and how to perform common file operations."
  },
  {
    "objectID": "5. Exploring the shell environment.html",
    "href": "5. Exploring the shell environment.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Now that you’ve become acquainted with launching your command line interface and using shell, let’s delve into the practical aspects - managing your files and directories.\n\n\n\nOur first task is to master navigation on the command line:\n\nType cd (short for “change directory”) followed by a directory name, such as cd Documents, and press Enter. We will go over how to move up the directory tree later in this section.\nIf you ever find yourself disoriented and need to return to your starting point, simply type cd without any additional arguments and press Enter. This will take you to your home directory.\n\n\n\n\nIf you are not sure what directory you are in and what the directory tree is you can use this command:\n\nType pwd (short for “print working directory”) and press Enter. This will print your absolute path - more on that later.\n\n\n\n\nNow let’s look at how to create and delete files and directories.\n\n\n\nTo create a new directory, type mkdir DirectoryName and press Enter. Replace DirectoryName with your desired directory name.\nTo generate a new text file, type touch FileName.txt and press Enter. Insert your chosen name after FileName. It’s akin to composing a new document.\n\n\n\n\n\nLet’s also learn how to delete files and directories:\n\nTo delete a file, type rm FileName.txt and press Enter. This is not reversable!\nTo delete an entire directory, type rm -r DirectoryName and press Enter. This is not reversable!\n\n\n\n\nNow, let’s learn how to move and copy files.\n\n\nTo duplicate a file, type cp SourceFile.txt DestinationDirectory/ and press Enter. Substitute SourceFile with the file to be duplicated and DestinationDirectory with the target directory. This will make a copy of SourceFile in the DestinationDirectory.\ncp SourceFile.txt DestinationDirectory/\n\n\n\nTo move a file instead, type mv SourceFile.txt DestinationDirectory/ and press Enter.\nmv SourceFile.txt DestinationDirectory/\n\n\n\n\nOccasionally, you may want to rename your files and directories:\nTo rename a file, we again use the mv command, except this this we give it put its new name after the file name instead of a destination directory.\nmv OldFileName.txt NewFileName.txt\nTo rename a directory, we do the same thing as with a file, but give it a new directory name:\nmv OldDirectoryName/ NewDirectoryName/\n\n\n\nIn your research, you’ll often need to inspect the content of files, whether they contain DNA sequences, metadata, or analysis results. Let’s explore some commonly used commands for viewing the contents of files within your command-line environment.\n\n\nText files are frequently used in bioinformatics for storing data and information. To view the contents of a text file, you can use the following commands:\n\ncat: This command displays the entire contents of a text file directly in your terminal. For instance, to view the contents of a file called sequence.txt, you would use:\n\ncat sequence.txt\n\nless: This command allows you to view large text files one screen at a time, making it easier to navigate through lengthy documents. To use less, simply type:\n\nless sequence.txt\nYou can navigate through the file using the arrow keys and exit by pressing q.\n\nhead and tail: These commands display the first or last few lines of a file, respectively. For example, to view the first 10 lines of a file:\n\nhead -n 10 sequence.txt\nTo view the last 20 lines of a file:\ntail -n 20 sequence.txt\n\n\n\nIf you want to search text files for a specific pattern or string we can use the bash command grep.\nLets assume we had a file called fruitprices.txt that looks like this:\nFruit   Price\nPineapple   40\nMango   60\nBanana  20\nPeach   25\nIf we wanted to check the price of mangos we would use:\ngrep \"mango\" fruitprices.txt\nWhich would return:\nMango 60\nAs we can see, grep returns the entire line that the search pattern is found in. By default it is not case-sensitive.\nIf fruitprices.txt looked like this instead:\nFruit   Price\nPineapple   40\nMango   60\nBanana  20\nPeach   25\nBig mango   70\nAnd we used the same command as above, it would return:\nMango 60\nBig mango 70\nNotice that grep returns every single line that contians the pattern/string we searched for. grep has a lot of different flags and options that changes its behaviour and make it a very powerful tool. Make sure to use --help to find out more.\n\n\n\n\nWhen navigating the command line, it’s crucial to understand how to specify the location of files and directories accurately. This is where the concepts of absolute and relative paths come into play.\n\n\nAn absolute path provides the complete and unambiguous location of a file or directory in the file system, starting from the root directory. The root directory is the top level of the directory tree and contains all directories below it. It is represented by a single forward slash (/).\nFor example, if your home directory is /home/username (the directory you enter when you type cd) and you want to view a file called data.txt located in the Documents directory within your home directory, you would use the absolute path like this:\nless /home/username/Documents/data.txt\nAbsolute paths are useful when you need to access files or directories from anywhere in the file system, regardless of your current working directory. They specify the precise location without any ambiguity.\n\n\n\nIn contrast, relative paths specify the location of a file or directory relative to your current working directory. They are typically shorter and more convenient to use, as we often don’t want to type the absolute path when a file in many directories deep.\nHere are some examples of relative paths:\n\nTo view the data.txt file in your current directory, you can simply use its name:\n\nless data.txt\n\nTo access a file or directory in a subdirectory, you can use a path relative to your current directory. For instance, to view the config.txt file in a directory called settings located within your current directory:\n\nless settings/config.txt\n\nTo move up one directory level, you can use ... For example, if you are in the settings directory and want to view a file in the parent directory:\n\nless ../parentfile.txt\nRelative paths are handy when you are working within a specific context, such as organizing files within a project directory.\nThese relative and absolute paths work with all commands not just ones to look at text files. For example, if you wanted to navigate around we use the cd command we introduced earlier.\nWe use pwd and find that we are in /home/username/Documents/amrflows/. pwd always prints out the absolute path. We want to move up a directory back into /home/username/Documents/.\n\nWe could move there using absolute paths:\n\ncd /home/username/Documents/\n\nor we could recognise that we only want to move up one directory and can use relative paths:\n\ncd ../\nSee how much shorter the command is. This becomes important when our commands become more complicated.\n\n\n\nYou can easily access previous commands you have executed by pressing the up arrow key. This will allow you to step back command-by-command and quickly rerun things or check what you’ve written.\n\n\n\n\nNote: this is only relevant if you’re connecting to a server. You can skip this section if you are running your commands locally on your own computer.\n\nIf you’ve connected to a server through SSH protocol, and you lose connection through internet failure, terminal crashing, or just closing your laptop/computer, you’ll lose all progress on what you’re currently working on. For example, if you are in the middle of running a program that takes hours or days to complete, you will have to start from the beginning.\nTo avoid this, we can use a program called screen. Screen is a virtual command line within a command line which continues to function even when connection is lost. To start a screen we can use the command:\nscreen -S &lt;screen_name&gt;\nYou can exit a screen using:\nscreen -d\nYou can get a list of the screen instances running on your server with:\nscreen -ls \nYou can reattach to your screen of choice with screen -r followed by the name or number of the screen given by screen -ls.\n\n\n\n\nThat was quite a bit to digest, but don’t fret if you can’t recall everything immediately. The most effective way to become proficient is through practice.\n\n\n\nMove into your home directory.\nCreate a new directory called “Metagenomics_Project.”\nMove into the new directory\nGenerate a new text file named “SampleData.txt.”\nMove the “SampleData.txt” file to your home directory (the starting location when you open your command line).\nRename the file to “MyData.txt.”\nList the contents of your home directory to verify that everything is in its rightful place.\n\n\n\nExample code for exercise\n\ncd\nmkdir Metagenomics_Project \ncd Metagenomics_Project \ntouch SampleData.txt \nmv SampleData.txt ../ \ncd ../\nmv SampleData.txt MyData.txt \nls\n\nCongratulations! You’ve just explored the fundamentals of managing files and directories using the command line. In the upcoming section, we’ll delve deeper into bioinformatic tools and how to install them."
  },
  {
    "objectID": "5. Exploring the shell environment.html#exploring-your-environment",
    "href": "5. Exploring the shell environment.html#exploring-your-environment",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Now that you’ve become acquainted with launching your command line interface and using shell, let’s delve into the practical aspects - managing your files and directories.\n\n\n\nOur first task is to master navigation on the command line:\n\nType cd (short for “change directory”) followed by a directory name, such as cd Documents, and press Enter. We will go over how to move up the directory tree later in this section.\nIf you ever find yourself disoriented and need to return to your starting point, simply type cd without any additional arguments and press Enter. This will take you to your home directory.\n\n\n\n\nIf you are not sure what directory you are in and what the directory tree is you can use this command:\n\nType pwd (short for “print working directory”) and press Enter. This will print your absolute path - more on that later.\n\n\n\n\nNow let’s look at how to create and delete files and directories.\n\n\n\nTo create a new directory, type mkdir DirectoryName and press Enter. Replace DirectoryName with your desired directory name.\nTo generate a new text file, type touch FileName.txt and press Enter. Insert your chosen name after FileName. It’s akin to composing a new document.\n\n\n\n\n\nLet’s also learn how to delete files and directories:\n\nTo delete a file, type rm FileName.txt and press Enter. This is not reversable!\nTo delete an entire directory, type rm -r DirectoryName and press Enter. This is not reversable!\n\n\n\n\nNow, let’s learn how to move and copy files.\n\n\nTo duplicate a file, type cp SourceFile.txt DestinationDirectory/ and press Enter. Substitute SourceFile with the file to be duplicated and DestinationDirectory with the target directory. This will make a copy of SourceFile in the DestinationDirectory.\ncp SourceFile.txt DestinationDirectory/\n\n\n\nTo move a file instead, type mv SourceFile.txt DestinationDirectory/ and press Enter.\nmv SourceFile.txt DestinationDirectory/\n\n\n\n\nOccasionally, you may want to rename your files and directories:\nTo rename a file, we again use the mv command, except this this we give it put its new name after the file name instead of a destination directory.\nmv OldFileName.txt NewFileName.txt\nTo rename a directory, we do the same thing as with a file, but give it a new directory name:\nmv OldDirectoryName/ NewDirectoryName/\n\n\n\nIn your research, you’ll often need to inspect the content of files, whether they contain DNA sequences, metadata, or analysis results. Let’s explore some commonly used commands for viewing the contents of files within your command-line environment.\n\n\nText files are frequently used in bioinformatics for storing data and information. To view the contents of a text file, you can use the following commands:\n\ncat: This command displays the entire contents of a text file directly in your terminal. For instance, to view the contents of a file called sequence.txt, you would use:\n\ncat sequence.txt\n\nless: This command allows you to view large text files one screen at a time, making it easier to navigate through lengthy documents. To use less, simply type:\n\nless sequence.txt\nYou can navigate through the file using the arrow keys and exit by pressing q.\n\nhead and tail: These commands display the first or last few lines of a file, respectively. For example, to view the first 10 lines of a file:\n\nhead -n 10 sequence.txt\nTo view the last 20 lines of a file:\ntail -n 20 sequence.txt\n\n\n\nIf you want to search text files for a specific pattern or string we can use the bash command grep.\nLets assume we had a file called fruitprices.txt that looks like this:\nFruit   Price\nPineapple   40\nMango   60\nBanana  20\nPeach   25\nIf we wanted to check the price of mangos we would use:\ngrep \"mango\" fruitprices.txt\nWhich would return:\nMango 60\nAs we can see, grep returns the entire line that the search pattern is found in. By default it is not case-sensitive.\nIf fruitprices.txt looked like this instead:\nFruit   Price\nPineapple   40\nMango   60\nBanana  20\nPeach   25\nBig mango   70\nAnd we used the same command as above, it would return:\nMango 60\nBig mango 70\nNotice that grep returns every single line that contians the pattern/string we searched for. grep has a lot of different flags and options that changes its behaviour and make it a very powerful tool. Make sure to use --help to find out more.\n\n\n\n\nWhen navigating the command line, it’s crucial to understand how to specify the location of files and directories accurately. This is where the concepts of absolute and relative paths come into play.\n\n\nAn absolute path provides the complete and unambiguous location of a file or directory in the file system, starting from the root directory. The root directory is the top level of the directory tree and contains all directories below it. It is represented by a single forward slash (/).\nFor example, if your home directory is /home/username (the directory you enter when you type cd) and you want to view a file called data.txt located in the Documents directory within your home directory, you would use the absolute path like this:\nless /home/username/Documents/data.txt\nAbsolute paths are useful when you need to access files or directories from anywhere in the file system, regardless of your current working directory. They specify the precise location without any ambiguity.\n\n\n\nIn contrast, relative paths specify the location of a file or directory relative to your current working directory. They are typically shorter and more convenient to use, as we often don’t want to type the absolute path when a file in many directories deep.\nHere are some examples of relative paths:\n\nTo view the data.txt file in your current directory, you can simply use its name:\n\nless data.txt\n\nTo access a file or directory in a subdirectory, you can use a path relative to your current directory. For instance, to view the config.txt file in a directory called settings located within your current directory:\n\nless settings/config.txt\n\nTo move up one directory level, you can use ... For example, if you are in the settings directory and want to view a file in the parent directory:\n\nless ../parentfile.txt\nRelative paths are handy when you are working within a specific context, such as organizing files within a project directory.\nThese relative and absolute paths work with all commands not just ones to look at text files. For example, if you wanted to navigate around we use the cd command we introduced earlier.\nWe use pwd and find that we are in /home/username/Documents/amrflows/. pwd always prints out the absolute path. We want to move up a directory back into /home/username/Documents/.\n\nWe could move there using absolute paths:\n\ncd /home/username/Documents/\n\nor we could recognise that we only want to move up one directory and can use relative paths:\n\ncd ../\nSee how much shorter the command is. This becomes important when our commands become more complicated.\n\n\n\nYou can easily access previous commands you have executed by pressing the up arrow key. This will allow you to step back command-by-command and quickly rerun things or check what you’ve written.\n\n\n\n\nNote: this is only relevant if you’re connecting to a server. You can skip this section if you are running your commands locally on your own computer.\n\nIf you’ve connected to a server through SSH protocol, and you lose connection through internet failure, terminal crashing, or just closing your laptop/computer, you’ll lose all progress on what you’re currently working on. For example, if you are in the middle of running a program that takes hours or days to complete, you will have to start from the beginning.\nTo avoid this, we can use a program called screen. Screen is a virtual command line within a command line which continues to function even when connection is lost. To start a screen we can use the command:\nscreen -S &lt;screen_name&gt;\nYou can exit a screen using:\nscreen -d\nYou can get a list of the screen instances running on your server with:\nscreen -ls \nYou can reattach to your screen of choice with screen -r followed by the name or number of the screen given by screen -ls.\n\n\n\n\nThat was quite a bit to digest, but don’t fret if you can’t recall everything immediately. The most effective way to become proficient is through practice.\n\n\n\nMove into your home directory.\nCreate a new directory called “Metagenomics_Project.”\nMove into the new directory\nGenerate a new text file named “SampleData.txt.”\nMove the “SampleData.txt” file to your home directory (the starting location when you open your command line).\nRename the file to “MyData.txt.”\nList the contents of your home directory to verify that everything is in its rightful place.\n\n\n\nExample code for exercise\n\ncd\nmkdir Metagenomics_Project \ncd Metagenomics_Project \ntouch SampleData.txt \nmv SampleData.txt ../ \ncd ../\nmv SampleData.txt MyData.txt \nls\n\nCongratulations! You’ve just explored the fundamentals of managing files and directories using the command line. In the upcoming section, we’ll delve deeper into bioinformatic tools and how to install them."
  },
  {
    "objectID": "6. Installing tools with Conda.html",
    "href": "6. Installing tools with Conda.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In the world of metagenomics, having the right tools at your disposal is crucial. Conda, short for Anaconda or Miniconda, is a powerful package manager and environment manager that simplifies the installation and management of bioinformatic tools. It’s like having a toolbox filled with specialized instruments for your metagenomic projects. Why Conda?\n\nSimplicity: Conda streamlines the process of installing and managing bioinformatics tools. You won’t need to hunt for individual software packages and dependencies or worry about compatibility issues.\nIsolation: Conda creates isolated environments for your projects, ensuring that different tools and their dependencies don’t interfere with each other. It’s like having separate workbenches for different experiments.\nCross-Platform: Conda works on Windows, macOS, and Linux, making it accessible to all users regardless of their operating system.\n\n\n\n\nBefore we dive into installing bioinformatic tools, let’s ensure you have Conda set up on your system. Follow these steps: Step 1: Download Miniconda\n\nVisit the Miniconda website (https://docs.conda.io/en/latest/miniconda.html).\nDownload the installer for your operating system (Windows, macOS, or Linux).\nRun the installer and follow the on-screen instructions. You can choose to install it just for yourself or for all users on your system.\n\nStep 2: Test Your Installation\nTo confirm that Conda is installed correctly, open a new command line or terminal window and type:\nconda --version\nYou should see the Conda version number displayed. If you encounter any issues, refer to the Conda documentation for troubleshooting or send me an email.\n\n\n\nOne of the key benefits of Conda is the ability to create isolated environments for different projects. Each environment can have its own set of tools and dependencies without interfering with others.\n\n\nThe decision to create a new Conda environment depends on various factors:\n\nTool Compatibility: If you’re working on different projects that require different versions of the same tool or conflicting dependencies, it’s a good idea to create separate environments.\nProject Isolation: Environments ensure that changes made for one project don’t affect others. If you’re collaborating with others or managing multiple research projects, isolate them in separate environments.\nDependency Management: Some tools may have specific dependencies or library requirements. Environments allow you to manage these dependencies independently.\nVersion Control: If you need to freeze the tool versions for a project to maintain consistency, you can do so within a Conda environment.\n\n\n\n\nTo create a Conda environment, use the following command, replacing myenv with your desired environment name:\nconda create --name myenv\nYou can also specify the Python version by adding python=X.X to the command. This may be important for certain tools. For example, to create an environment with Python 3.8:\nconda create --name myenv python=3.8\nActivating and Deactivating Environments\nWhen you start a new project, you need to “activate” its workspace (Conda environment). It’s like opening your toolbox for that specific project. To activate a conda environment you use the following command:\nconda activate myenv\nReplace myenv with the name of your project’s workspace. Now, when you use commands, you’re working within this special workspace with its own set of coding tools and libraries.\nIn order to see what packages/tools have been install in the environment you are currently in, you can use:\nconda list\nYou can also see a list of all your different conda environments by using:\nconda env list\nWith the command above, the environment you are currently in will be marked with an asterisk (*).\nWhen you’re done with a project or want to switch to another, you “deactivate” the current workspace. This is like closing your toolbox so that you don’t accidentally mix up your tools. It’s important to do this befor activating a different conda environment. Just type this:\nconda deactivate\nYou’ll be back to your regular, default workspace (called the base environment), where you can start a new project or do other tasks.\n\n\n\nNow that you have Conda and understand how to manage environments, let’s install some bioinformatic tools.\nTo install a bioinformatic tool, activate your desired Conda environment and use the conda install command. For example, to install the popular tool “fastp”:\nhttps://github.com/OpenGene/fastp#fastp\nconda activate myenv\nconda install -c bioconda fastp\nReplace myenv with your environment name and adjust the tool name as needed.\nWe use -c bioconda because we’re telling conda to search the bioconda channel for the tool we’re looking for. Channels are like marketplaces for software, and bioconda is dedicated to tools designed for bioinformaticians. This makes it easier for conda to find the specificied package. This information is normally found on the tools github page or on their anaconda page:\nhttps://anaconda.org/bioconda/fastp\n\n\n\nTo keep your tools up to date, periodically update your Conda packages:\nconda update --all\n\n\n\n\nIn this section, you’ve learned the power of Conda for managing bioinformatic tools. You’ve set up Conda on your system, created environments, and installed a tool. Now, practice installing other bioinformatic tools relevant to your metagenomics research.\n\n\n\nCreate a new directory for a different metagenomic project, and consider whether it needs its own Conda environment based on the factors mentioned earlier.\nActivate the appropriate environment or create a new one if needed.\nInstall a bioinformatic tool related to your new project using Conda.\nVerify that the tool is installed and operational within your environment.\n\n\n\nExample code for exercise\n\nIn this example I install a metagenomic assembler called spades as an example. More details about spades can be found at it’s github page but we will cover this tool later in the course.\ncd \nmkdir new_project\nconda deactivate\nconda create --name spades_env -c bioconda spades\nconda activate spades_env\nspades.py --version\n\nWith Conda, you’re now equipped to efficiently install and manage the tools you need for your metagenomic analyses. In the next section, we’ll delve deeper into specific metagenomic tools and workflows.\nExtra reading: Mamba is a reimplementation of conda in C++. It is much faster at dependency solving and downloading of packages, and can be subsituted instead of Conda. We be won’t be mentioning it any further in this course to avoid confusion but you can read more about it in the following link if you are interested."
  },
  {
    "objectID": "6. Installing tools with Conda.html#installing-tools-with-conda",
    "href": "6. Installing tools with Conda.html#installing-tools-with-conda",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In the world of metagenomics, having the right tools at your disposal is crucial. Conda, short for Anaconda or Miniconda, is a powerful package manager and environment manager that simplifies the installation and management of bioinformatic tools. It’s like having a toolbox filled with specialized instruments for your metagenomic projects. Why Conda?\n\nSimplicity: Conda streamlines the process of installing and managing bioinformatics tools. You won’t need to hunt for individual software packages and dependencies or worry about compatibility issues.\nIsolation: Conda creates isolated environments for your projects, ensuring that different tools and their dependencies don’t interfere with each other. It’s like having separate workbenches for different experiments.\nCross-Platform: Conda works on Windows, macOS, and Linux, making it accessible to all users regardless of their operating system.\n\n\n\n\nBefore we dive into installing bioinformatic tools, let’s ensure you have Conda set up on your system. Follow these steps: Step 1: Download Miniconda\n\nVisit the Miniconda website (https://docs.conda.io/en/latest/miniconda.html).\nDownload the installer for your operating system (Windows, macOS, or Linux).\nRun the installer and follow the on-screen instructions. You can choose to install it just for yourself or for all users on your system.\n\nStep 2: Test Your Installation\nTo confirm that Conda is installed correctly, open a new command line or terminal window and type:\nconda --version\nYou should see the Conda version number displayed. If you encounter any issues, refer to the Conda documentation for troubleshooting or send me an email.\n\n\n\nOne of the key benefits of Conda is the ability to create isolated environments for different projects. Each environment can have its own set of tools and dependencies without interfering with others.\n\n\nThe decision to create a new Conda environment depends on various factors:\n\nTool Compatibility: If you’re working on different projects that require different versions of the same tool or conflicting dependencies, it’s a good idea to create separate environments.\nProject Isolation: Environments ensure that changes made for one project don’t affect others. If you’re collaborating with others or managing multiple research projects, isolate them in separate environments.\nDependency Management: Some tools may have specific dependencies or library requirements. Environments allow you to manage these dependencies independently.\nVersion Control: If you need to freeze the tool versions for a project to maintain consistency, you can do so within a Conda environment.\n\n\n\n\nTo create a Conda environment, use the following command, replacing myenv with your desired environment name:\nconda create --name myenv\nYou can also specify the Python version by adding python=X.X to the command. This may be important for certain tools. For example, to create an environment with Python 3.8:\nconda create --name myenv python=3.8\nActivating and Deactivating Environments\nWhen you start a new project, you need to “activate” its workspace (Conda environment). It’s like opening your toolbox for that specific project. To activate a conda environment you use the following command:\nconda activate myenv\nReplace myenv with the name of your project’s workspace. Now, when you use commands, you’re working within this special workspace with its own set of coding tools and libraries.\nIn order to see what packages/tools have been install in the environment you are currently in, you can use:\nconda list\nYou can also see a list of all your different conda environments by using:\nconda env list\nWith the command above, the environment you are currently in will be marked with an asterisk (*).\nWhen you’re done with a project or want to switch to another, you “deactivate” the current workspace. This is like closing your toolbox so that you don’t accidentally mix up your tools. It’s important to do this befor activating a different conda environment. Just type this:\nconda deactivate\nYou’ll be back to your regular, default workspace (called the base environment), where you can start a new project or do other tasks.\n\n\n\nNow that you have Conda and understand how to manage environments, let’s install some bioinformatic tools.\nTo install a bioinformatic tool, activate your desired Conda environment and use the conda install command. For example, to install the popular tool “fastp”:\nhttps://github.com/OpenGene/fastp#fastp\nconda activate myenv\nconda install -c bioconda fastp\nReplace myenv with your environment name and adjust the tool name as needed.\nWe use -c bioconda because we’re telling conda to search the bioconda channel for the tool we’re looking for. Channels are like marketplaces for software, and bioconda is dedicated to tools designed for bioinformaticians. This makes it easier for conda to find the specificied package. This information is normally found on the tools github page or on their anaconda page:\nhttps://anaconda.org/bioconda/fastp\n\n\n\nTo keep your tools up to date, periodically update your Conda packages:\nconda update --all\n\n\n\n\nIn this section, you’ve learned the power of Conda for managing bioinformatic tools. You’ve set up Conda on your system, created environments, and installed a tool. Now, practice installing other bioinformatic tools relevant to your metagenomics research.\n\n\n\nCreate a new directory for a different metagenomic project, and consider whether it needs its own Conda environment based on the factors mentioned earlier.\nActivate the appropriate environment or create a new one if needed.\nInstall a bioinformatic tool related to your new project using Conda.\nVerify that the tool is installed and operational within your environment.\n\n\n\nExample code for exercise\n\nIn this example I install a metagenomic assembler called spades as an example. More details about spades can be found at it’s github page but we will cover this tool later in the course.\ncd \nmkdir new_project\nconda deactivate\nconda create --name spades_env -c bioconda spades\nconda activate spades_env\nspades.py --version\n\nWith Conda, you’re now equipped to efficiently install and manage the tools you need for your metagenomic analyses. In the next section, we’ll delve deeper into specific metagenomic tools and workflows.\nExtra reading: Mamba is a reimplementation of conda in C++. It is much faster at dependency solving and downloading of packages, and can be subsituted instead of Conda. We be won’t be mentioning it any further in this course to avoid confusion but you can read more about it in the following link if you are interested."
  },
  {
    "objectID": "7. Sequencing-filetypes.html",
    "href": "7. Sequencing-filetypes.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Raw sequencing reads are typically provided in the FASTQ format. This standardised format is composed of four lines of information. Here’s what a single read looks likke in fastq format.\n@SEQ_ID\nCTGAGTTTTTAACTAAATAGTCTAAAAATGGTTTTCGTTT\n+\n!(((**+%%''.1--))++(%)ABFFFFF&lt;&lt;99GG00//.\n\nLine 1 contains the @ prefix, and contains the name/title/description of the sequence. Typically this is the read ID generated by the sequencing device and the name of your sample.\nLine 2 is the raw sequence data, i.e. the individual nucleotides that have been basecalled.\nLine 3 contains the + prefix, this is normally empty in modern sequencing, but may repeat the information from line 1.\nLine 4 contains the ASCII encoded quality score for each basecalled nucleotide in line 2.\n\nFASTQ files contains all the reads relating to a sequencing sample. If we had 10,000 reads we would have 4 x 10,000 = 40,000 lines of text. If we want to find out how many reads are in our file we can use the word count by line command and divide by 4:\nwc -l reads.fastq\n\n\nIn Illumina sequencing, each base in a read is assigned a Phred score, which are included in the FASTQ file along with the sequence data. A Phred score is a numerical value that quantifies the probability of an incorrect base call at a specific position in a DNA sequence. It is expressed on a logarithmic scale and is sometimes known as a Q-score. A higher phred score indicates a higher level of confidence in the accuracy of a base call. for example, a Phred score of 30 means that there is a 1 in 1,000 chance (0.1%) of the base call being incorrect. A Phred score of 20 corresponds to a 1 in 100 chance (1%) of a base call being wrong. Based on the quality scores, sequences can be trimmed or filtered to improve the overall quality of the dataset.\nIllumina sequencing typically produces sequences with an average Phred score of 30+. Quality scores are encoded in ASCII characters as integers have varying length. We don’t need to worry about interperting these as programs do this automaticlly.\n\n\n\nSequencing data is often “paired-end” meaning there will be two fastq files - one for all the 5’ - 3’ oriented reads and one for the 3’ -5’ reads. These files are typically delinated by the suffixes _R1 and R2 or simply _1 and _2. These fastq files contain lots of information and therefore can be very large. This can be mitigated through compression.\nThe most common method for compressing FASTQ files is g-zip. FASTQ files you recieve from the sequencer or that you download are often already compressed this way, which is indicated by the .gz extension. You can compress your own files (as long as they haven’t been compressed already) with:\ngzip reads.fastq\nThis will convert the reads.fastq file into reads.fastq.gz in your working directory. To decompress the gzipped file, we simply:\ngunzip reads.fastq.gzip\nUnzipping is often unnecassary as most programs are built to handle gzipped files, so its best to leave them compressed to save space!\n\n\n\n\nFASTA files are similar to FASTQ files but only contain two lines instead of four:\n\nID ACTGATTGACTAGCAGTTTTGGACAGA\n\n\nLine 1 always contains the “&gt;” prefix, followed by the title or description of the sequence beneath\nLine 2 contains the sequence data\n\nFASTA differs from FASTQ as there is no quality information included. FASTA files are typically used for assemblies (which we will cover later) with each contig defined by the &gt; character.\nSometimes, reads are converted to FASTA format for certain tools but this is uncommon. The general rule is that fastq files are for reads and fasta files are for assemblies."
  },
  {
    "objectID": "7. Sequencing-filetypes.html#sequencing-filetypes",
    "href": "7. Sequencing-filetypes.html#sequencing-filetypes",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Raw sequencing reads are typically provided in the FASTQ format. This standardised format is composed of four lines of information. Here’s what a single read looks likke in fastq format.\n@SEQ_ID\nCTGAGTTTTTAACTAAATAGTCTAAAAATGGTTTTCGTTT\n+\n!(((**+%%''.1--))++(%)ABFFFFF&lt;&lt;99GG00//.\n\nLine 1 contains the @ prefix, and contains the name/title/description of the sequence. Typically this is the read ID generated by the sequencing device and the name of your sample.\nLine 2 is the raw sequence data, i.e. the individual nucleotides that have been basecalled.\nLine 3 contains the + prefix, this is normally empty in modern sequencing, but may repeat the information from line 1.\nLine 4 contains the ASCII encoded quality score for each basecalled nucleotide in line 2.\n\nFASTQ files contains all the reads relating to a sequencing sample. If we had 10,000 reads we would have 4 x 10,000 = 40,000 lines of text. If we want to find out how many reads are in our file we can use the word count by line command and divide by 4:\nwc -l reads.fastq\n\n\nIn Illumina sequencing, each base in a read is assigned a Phred score, which are included in the FASTQ file along with the sequence data. A Phred score is a numerical value that quantifies the probability of an incorrect base call at a specific position in a DNA sequence. It is expressed on a logarithmic scale and is sometimes known as a Q-score. A higher phred score indicates a higher level of confidence in the accuracy of a base call. for example, a Phred score of 30 means that there is a 1 in 1,000 chance (0.1%) of the base call being incorrect. A Phred score of 20 corresponds to a 1 in 100 chance (1%) of a base call being wrong. Based on the quality scores, sequences can be trimmed or filtered to improve the overall quality of the dataset.\nIllumina sequencing typically produces sequences with an average Phred score of 30+. Quality scores are encoded in ASCII characters as integers have varying length. We don’t need to worry about interperting these as programs do this automaticlly.\n\n\n\nSequencing data is often “paired-end” meaning there will be two fastq files - one for all the 5’ - 3’ oriented reads and one for the 3’ -5’ reads. These files are typically delinated by the suffixes _R1 and R2 or simply _1 and _2. These fastq files contain lots of information and therefore can be very large. This can be mitigated through compression.\nThe most common method for compressing FASTQ files is g-zip. FASTQ files you recieve from the sequencer or that you download are often already compressed this way, which is indicated by the .gz extension. You can compress your own files (as long as they haven’t been compressed already) with:\ngzip reads.fastq\nThis will convert the reads.fastq file into reads.fastq.gz in your working directory. To decompress the gzipped file, we simply:\ngunzip reads.fastq.gzip\nUnzipping is often unnecassary as most programs are built to handle gzipped files, so its best to leave them compressed to save space!\n\n\n\n\nFASTA files are similar to FASTQ files but only contain two lines instead of four:\n\nID ACTGATTGACTAGCAGTTTTGGACAGA\n\n\nLine 1 always contains the “&gt;” prefix, followed by the title or description of the sequence beneath\nLine 2 contains the sequence data\n\nFASTA differs from FASTQ as there is no quality information included. FASTA files are typically used for assemblies (which we will cover later) with each contig defined by the &gt; character.\nSometimes, reads are converted to FASTA format for certain tools but this is uncommon. The general rule is that fastq files are for reads and fasta files are for assemblies."
  },
  {
    "objectID": "8. Read Quality Control.html",
    "href": "8. Read Quality Control.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In this section we will use our first bioinformatics tool, fastp. Before we start I want to repeat that the number one skill that you need to learn as a bioinformatian is to be able to read and understand software manuals. Manuals are available on the software’s website or on their GitHub page. Step one of running a new tool is to always read the entire manual. Not reading the full manual leads to a lot of mistakes that waste computational time. If you run into any issues, the first time to do is to read the manual again. Programs often include a help section that is accessible on the command line. For the vast majority of the tools, you just type the name of the program followed by the flag -h, --help, or -help. This is very useful for refreshing our memory on what different flags are and what they do in that particular program.\n\n\nThe tool fastp combines quality assessment and read processing, streamlining your analysis pipeline. There are also tools such as fastqc, prinseq, trimmomatic etc., but I have a preference for fastp because:\n\nAll-in-One: fastp performs both quality control and preprocessing tasks, reducing the need for multiple tools.\nSpeed: As the name suggests, fastp is designed for speed, making it efficient for processing large datasets.\nAdapter Trimming: fastp also performs automatic adapter trimming and removes unwanted sequences from your reads.\n\n\n\n\nBefore we start we need to ensure that fastp is installed in your Conda environment. If not already done, activate the desired environment and install fastp using this command:\nconda install -c bioconda fastp\nYou can also create a conda environment specifically for fastp to ensure there are no dependency clashes.\nconda create --name fastp_env -c bioconda fastp\nconda activate fastp_env\nWith fastp installed, you’re ready to assess the quality of your sequencing data.\n\n\n\nStep 1: Prepare Your Data * Open a command line or terminal window. * Gather your raw sequencing data files and place them in a directory.\nStep 2: Run fastp * Run fastp with the following command (replace input.fastq and output.fastq with your filenames):\nfastp -i input.fastq -o output.fastq -h output.html\nfastp will analyze the data, generate a quality report in html format, and preprocess the reads into output.fastq.\n\nNote: We’ll go through how to run fastp on multiple files and on paired-end reads in later sections.\n\n\n\n\nAs part of its functionality, fastp produces quality control reports that provide insight into the state of your data. The reports are generated in HTML format and can be viewed in a web browser.\nOpen the HTML report in a web browser to explore key metrics:\n\nAdapter content\nDuplication rate\nTotal reads/bases before and after filtering\nRead quality (Q20 and Q30) before and aftering filtering\nGC ratio\nMean length\n\nBy default, fastp removes reads with over 40% of bases below a phred quality of 15. The phred quality threshold can be adjusted with the -q flag. It also automatically trims adaptor sequences by overlap analysis. If you know the adaptor sequence for your reads you can specify them after the a flag. fastp also has other features, such as global trimming, read deduplication, overrepresented sequence analysis, and read merging. Details of all of fastp’s features and how to use the additonal modes can be found at: https://github.com/OpenGene/fastp\nFor the majority of Illumina metagenomic reads, running through fastp’s default settings will be sufficient for producing clean input data for downstream analysis. However, it is important to manually inspect the html output from fastp in order to spot any potential issues.\n\n\n\nIn this section, you’ve explored read quality control and preprocessing using the fastp tool. You’ve gained insights into installing fastp, running quality control and preprocessing, and interpreting quality reports. Now, it’s time to apply this knowledge to your own metagenomic data.\n\n\n\n\nActivate your Conda environment.\nNavigate to the directory containing your raw sequencing data files.\nRun fastp on one fastq file.\nOpen and examine the generated fastp quality control report in a web browser.\n\n\n\nExample code for exercise\n\nconda activate fastp_env\ncd sequencing_data\nfastp -i input.fastq -o output.fastq -h output.html\n\nBy incorporating fastp into your workflow, you’ve taken a important step towards ensuring accurate and reliable metagenomic analyses. In the next sections, we’ll explore how to run fastp on multiple fastq files, as well as on paired-end data."
  },
  {
    "objectID": "8. Read Quality Control.html#running-your-first-bioinformatics-tool---fastp",
    "href": "8. Read Quality Control.html#running-your-first-bioinformatics-tool---fastp",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In this section we will use our first bioinformatics tool, fastp. Before we start I want to repeat that the number one skill that you need to learn as a bioinformatian is to be able to read and understand software manuals. Manuals are available on the software’s website or on their GitHub page. Step one of running a new tool is to always read the entire manual. Not reading the full manual leads to a lot of mistakes that waste computational time. If you run into any issues, the first time to do is to read the manual again. Programs often include a help section that is accessible on the command line. For the vast majority of the tools, you just type the name of the program followed by the flag -h, --help, or -help. This is very useful for refreshing our memory on what different flags are and what they do in that particular program.\n\n\nThe tool fastp combines quality assessment and read processing, streamlining your analysis pipeline. There are also tools such as fastqc, prinseq, trimmomatic etc., but I have a preference for fastp because:\n\nAll-in-One: fastp performs both quality control and preprocessing tasks, reducing the need for multiple tools.\nSpeed: As the name suggests, fastp is designed for speed, making it efficient for processing large datasets.\nAdapter Trimming: fastp also performs automatic adapter trimming and removes unwanted sequences from your reads.\n\n\n\n\nBefore we start we need to ensure that fastp is installed in your Conda environment. If not already done, activate the desired environment and install fastp using this command:\nconda install -c bioconda fastp\nYou can also create a conda environment specifically for fastp to ensure there are no dependency clashes.\nconda create --name fastp_env -c bioconda fastp\nconda activate fastp_env\nWith fastp installed, you’re ready to assess the quality of your sequencing data.\n\n\n\nStep 1: Prepare Your Data * Open a command line or terminal window. * Gather your raw sequencing data files and place them in a directory.\nStep 2: Run fastp * Run fastp with the following command (replace input.fastq and output.fastq with your filenames):\nfastp -i input.fastq -o output.fastq -h output.html\nfastp will analyze the data, generate a quality report in html format, and preprocess the reads into output.fastq.\n\nNote: We’ll go through how to run fastp on multiple files and on paired-end reads in later sections.\n\n\n\n\nAs part of its functionality, fastp produces quality control reports that provide insight into the state of your data. The reports are generated in HTML format and can be viewed in a web browser.\nOpen the HTML report in a web browser to explore key metrics:\n\nAdapter content\nDuplication rate\nTotal reads/bases before and after filtering\nRead quality (Q20 and Q30) before and aftering filtering\nGC ratio\nMean length\n\nBy default, fastp removes reads with over 40% of bases below a phred quality of 15. The phred quality threshold can be adjusted with the -q flag. It also automatically trims adaptor sequences by overlap analysis. If you know the adaptor sequence for your reads you can specify them after the a flag. fastp also has other features, such as global trimming, read deduplication, overrepresented sequence analysis, and read merging. Details of all of fastp’s features and how to use the additonal modes can be found at: https://github.com/OpenGene/fastp\nFor the majority of Illumina metagenomic reads, running through fastp’s default settings will be sufficient for producing clean input data for downstream analysis. However, it is important to manually inspect the html output from fastp in order to spot any potential issues.\n\n\n\nIn this section, you’ve explored read quality control and preprocessing using the fastp tool. You’ve gained insights into installing fastp, running quality control and preprocessing, and interpreting quality reports. Now, it’s time to apply this knowledge to your own metagenomic data.\n\n\n\n\nActivate your Conda environment.\nNavigate to the directory containing your raw sequencing data files.\nRun fastp on one fastq file.\nOpen and examine the generated fastp quality control report in a web browser.\n\n\n\nExample code for exercise\n\nconda activate fastp_env\ncd sequencing_data\nfastp -i input.fastq -o output.fastq -h output.html\n\nBy incorporating fastp into your workflow, you’ve taken a important step towards ensuring accurate and reliable metagenomic analyses. In the next sections, we’ll explore how to run fastp on multiple fastq files, as well as on paired-end data."
  },
  {
    "objectID": "9. For-loops.html",
    "href": "9. For-loops.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Now that you’ve learned how to use fastp to perform read quality control and preprocessing on a single file, let’s explore how to scale up your analysis to process multiple files efficiently. This is where the concept of “for loops” comes into play.\nA for loop is like a magical spell in coding. It allows you to repeat a sequence of commands for each item in a list, making it perfect for automating repetitive tasks, such as processing multiple read files. Think of it as a conveyor belt that moves each file through the fastp process one by one.\n\n\nStep 1: Organize Your Files\nBefore you start, make sure all your raw sequencing data files are in the same directory. This directory should contain all the files you want to process.\nStep 2: Writing the Loop\nOpen a command line or terminal window and navigate to the directory where your files are located. Make sure you are in your fastp conda environment. Now, let’s write a for loop to run fastp on all the files in the directory.\nfor file in *.fastq; do fastp -i ${file} -o processed_${file}; done\nLets break down what each part of the for loop does:\n\nfor file in *.fastq: This line tells the loop to go through each file with the “.fastq” extension in the current directory. The asterisk is a wildcard. This means that any string can come before “.fastq”.\ndo: Marks the beginning of the loop.\nfastp -i ${file} -o processed_${file}: This is the fastp command, just like the one you used before. ${file} takes each file that we specified before (the ones that end in “.fastq”) and places it into the fastp command. The output files will be named as the fastq file name with the prefix “processed_”.\ndone: Marks the end of the loop.\n\nWhen you run this loop, it will automatically process each .fastq file in the directory, one after the other. You don’t have to type the fastp command for each file manually. This is a huge time-saver, especially when you have many files to process.\nNote: You can replace “file” with any name you like; it’s just a variable that represents each file in the list. For example the following for loop works identically:\nfor capybaras in *.fastq; do fastp -i ${capybaras} -o processed_${capybaras}; done\nIts normally easier to use simple variables like “file” or just single letters like “f” or “i”.\nYou can use the echo command to see what commands the for loop is iterating. You place echo after the do and encase the command in quotes:\nfor file in *.fastq; do echo \"fastp -i ${file} -o processed_${file}\"; done\n\n\n\nIn this section, you’ve discovered the power of for loops in automating repetitive tasks, such as read quality control. You’ve learned how to use a simple for loop to apply the fastp command to multiple files at once, and you’ve used the echo command to keep track of the processing.\nNow, it’s time to put your knowledge into practice:\n\n\n\nActivate your fastp conda environment.\nNavigate to the directory containing your raw sequencing data files.\nWrite a for loop like the ones above into your command line, using the variable name “f”, echoing your fastp command, and name the fastp output with the prefix “fastp_”.\nIf the echoed command looks correct, remove the echo and quotes and run the command.\nLet the loop do its magic and process all your .fastq files automatically while displaying processing messages. This might take awhile.\n\n\n\nExample code for exercise\n\nconda activate fastp_env\ncd sequencing_data\nfor file in *.fastq; do echo \"fastp -i ${file} -o processed_${file}\"; done\n\nOnce the loop finishes, you’ll have fastp outputs for each fastq with a “fastq_” prefix. Feel free to examine them and ensure that the quality control and preprocessing were successful.\nBy mastering the use of for loops and customizing variable names, you’ve unlocked a powerful tool for automating data analysis in metagenomics. In the next section, we’ll build on this for loop and use it to process paired end sequencing data."
  },
  {
    "objectID": "9. For-loops.html#automating-read-quality-control-with-for-loops",
    "href": "9. For-loops.html#automating-read-quality-control-with-for-loops",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Now that you’ve learned how to use fastp to perform read quality control and preprocessing on a single file, let’s explore how to scale up your analysis to process multiple files efficiently. This is where the concept of “for loops” comes into play.\nA for loop is like a magical spell in coding. It allows you to repeat a sequence of commands for each item in a list, making it perfect for automating repetitive tasks, such as processing multiple read files. Think of it as a conveyor belt that moves each file through the fastp process one by one.\n\n\nStep 1: Organize Your Files\nBefore you start, make sure all your raw sequencing data files are in the same directory. This directory should contain all the files you want to process.\nStep 2: Writing the Loop\nOpen a command line or terminal window and navigate to the directory where your files are located. Make sure you are in your fastp conda environment. Now, let’s write a for loop to run fastp on all the files in the directory.\nfor file in *.fastq; do fastp -i ${file} -o processed_${file}; done\nLets break down what each part of the for loop does:\n\nfor file in *.fastq: This line tells the loop to go through each file with the “.fastq” extension in the current directory. The asterisk is a wildcard. This means that any string can come before “.fastq”.\ndo: Marks the beginning of the loop.\nfastp -i ${file} -o processed_${file}: This is the fastp command, just like the one you used before. ${file} takes each file that we specified before (the ones that end in “.fastq”) and places it into the fastp command. The output files will be named as the fastq file name with the prefix “processed_”.\ndone: Marks the end of the loop.\n\nWhen you run this loop, it will automatically process each .fastq file in the directory, one after the other. You don’t have to type the fastp command for each file manually. This is a huge time-saver, especially when you have many files to process.\nNote: You can replace “file” with any name you like; it’s just a variable that represents each file in the list. For example the following for loop works identically:\nfor capybaras in *.fastq; do fastp -i ${capybaras} -o processed_${capybaras}; done\nIts normally easier to use simple variables like “file” or just single letters like “f” or “i”.\nYou can use the echo command to see what commands the for loop is iterating. You place echo after the do and encase the command in quotes:\nfor file in *.fastq; do echo \"fastp -i ${file} -o processed_${file}\"; done\n\n\n\nIn this section, you’ve discovered the power of for loops in automating repetitive tasks, such as read quality control. You’ve learned how to use a simple for loop to apply the fastp command to multiple files at once, and you’ve used the echo command to keep track of the processing.\nNow, it’s time to put your knowledge into practice:\n\n\n\nActivate your fastp conda environment.\nNavigate to the directory containing your raw sequencing data files.\nWrite a for loop like the ones above into your command line, using the variable name “f”, echoing your fastp command, and name the fastp output with the prefix “fastp_”.\nIf the echoed command looks correct, remove the echo and quotes and run the command.\nLet the loop do its magic and process all your .fastq files automatically while displaying processing messages. This might take awhile.\n\n\n\nExample code for exercise\n\nconda activate fastp_env\ncd sequencing_data\nfor file in *.fastq; do echo \"fastp -i ${file} -o processed_${file}\"; done\n\nOnce the loop finishes, you’ll have fastp outputs for each fastq with a “fastq_” prefix. Feel free to examine them and ensure that the quality control and preprocessing were successful.\nBy mastering the use of for loops and customizing variable names, you’ve unlocked a powerful tool for automating data analysis in metagenomics. In the next section, we’ll build on this for loop and use it to process paired end sequencing data."
  },
  {
    "objectID": "9.5. Paired-End-Data.html",
    "href": "9.5. Paired-End-Data.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In metagenomics, you often work with paired-end sequencing data, where each sample consists of two separate FASTQ files—one for forward reads and one for reverse reads. Fortunately, fastp can handle paired-end data efficiently when used in combination with for loops. Let’s dive into how you can set up and execute for loops for paired-end data, making use of variable string substitution.\n\n\nBefore we start, ensure that your paired-end FASTQ files are organized correctly. You should have a directory containing pairs of files, with one file for forward reads and another for reverse reads for each sample. The filenames should ideally follow a pattern to simplify automation. For example a set of pair reads would be named:\n\namrfA1_R1.fastq\namrfA1_R2.fastq\n\n\n\n\nTo process paired-end data with fastp using for loops, you’ll need to set up a loop that iterates over both the forward and reverse reads simultaneously. Here’s an example of how to do it:\nfor r in *_R1.fastq; do fastp -i ${r} -I ${r/R1/R2} -o ${r/R1.fastq/processed_R1.fastq} -O ${r/R1.fastq/processed_R2.fastq} -h ${r/R1.fastq/fastp.html};done\nNote that we use variable/parameter substitution here to change our variable r which contains the name of the forward read into the name of the reverse read. This works as follows:\n${parameter/pattern/replacement}\nSo if the parameter r represented amrfA1_R1.fastq the substitution ${r/R1/R2} would replace the pattern R1 with R2, resulting in amrfA1_R2.fastq.\nLet’s break down what each part of the loop does:\nfor r in *_R1.fastq: This line iterates through all files in the current directory that end with “_R1.fastq,” representing the forward reads. do fastp: This starts the for loop for fastp. -i ${r}: This inputs the files that end in “_R1.fastq” into fastp. -I ${r/R1/R2}: This line constructs the filename for the reverse reads by substituting “R1” with “R2” in the forward read variable. -o ${r/R1.fastq/processed_R1.fastq}: This line names the processed output forward reads by replacing the “R1.fastq” string from the forward read filename with “procressed_R1.fastq” -O ${r/R1.fastq/processed_R2.fastq}: This line names the processed output reverse reads by replacing the “R1.fastq” string from the forward read filename with “processed_R2.fastq” -h ${r/R1.fastq/fastp.html}: This line names the output html file by replacing the “R1.fastq” string from the forward read filename with “fastp.html” done: Ends the for loop.\nWhen you run this loop, it will automatically process each pair of forward and reverse reads in a single command. If we echo the above command, assuming amrfA1_R1.fastq and amrfA1_R2.fastq are the only files in our current directory, the output would be:\nfastp -i amrfA1_R1.fastq -I amrfA1_R2.fastq -o amrfA1_processed_R1.fastq -O amrfA1_processed_R2.fastq -h amrfA1_fastp.html\nOf course, just running the command above would get us the same result, but imagine doing this for hundreds of paired reads. For loops allow us to do this all in one command, instead of writing code for every set of paired reads.\n\n\n\nIn this section, you’ve learned how to use for loops with fastp to efficiently process paired-end sequencing data. Variable string substitution allows you to create dynamic filenames and automate the handling of both forward and reverse reads.\nExercise: Process Paired-End Data\n\nActivate your Conda environment.\nNavigate to the directory containing your paired-end sequencing data files.\nWrite a for loop like those provided above into your command line.\nRun the loop to process both forward and reverse reads.\nExamine the processed files, ensuring that the quality control and preprocessing were successful.\n\n\n\nExample code for exercise\n\nconda activate fastp_env\ncd sequencing_data\nfor r in *_R1.fastq; do fastp -i ${r} -I ${r/R1/R2} -o ${r/R1.fastq/processed_R1.fastq} -O ${r/R1.fastq/processed_R2.fastq} -h ${r/R1.fastq/fastp.html};done\n\nBy mastering the use of for loops and variable string substitution, you’re equipped to tackle paired-end data efficiently in your metagenomic analyses. There are other ways to manipulate variable strings in Bash, but substitution is by far the most used. If you want to read about other types of variable manipulation/expansion this wiki has a great table of various tricks:\nhttps://mywiki.wooledge.org/BashGuide/Parameters#Parameter_Expansion"
  },
  {
    "objectID": "9.5. Paired-End-Data.html#processing-paired-end-data-with-for-loops",
    "href": "9.5. Paired-End-Data.html#processing-paired-end-data-with-for-loops",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "In metagenomics, you often work with paired-end sequencing data, where each sample consists of two separate FASTQ files—one for forward reads and one for reverse reads. Fortunately, fastp can handle paired-end data efficiently when used in combination with for loops. Let’s dive into how you can set up and execute for loops for paired-end data, making use of variable string substitution.\n\n\nBefore we start, ensure that your paired-end FASTQ files are organized correctly. You should have a directory containing pairs of files, with one file for forward reads and another for reverse reads for each sample. The filenames should ideally follow a pattern to simplify automation. For example a set of pair reads would be named:\n\namrfA1_R1.fastq\namrfA1_R2.fastq\n\n\n\n\nTo process paired-end data with fastp using for loops, you’ll need to set up a loop that iterates over both the forward and reverse reads simultaneously. Here’s an example of how to do it:\nfor r in *_R1.fastq; do fastp -i ${r} -I ${r/R1/R2} -o ${r/R1.fastq/processed_R1.fastq} -O ${r/R1.fastq/processed_R2.fastq} -h ${r/R1.fastq/fastp.html};done\nNote that we use variable/parameter substitution here to change our variable r which contains the name of the forward read into the name of the reverse read. This works as follows:\n${parameter/pattern/replacement}\nSo if the parameter r represented amrfA1_R1.fastq the substitution ${r/R1/R2} would replace the pattern R1 with R2, resulting in amrfA1_R2.fastq.\nLet’s break down what each part of the loop does:\nfor r in *_R1.fastq: This line iterates through all files in the current directory that end with “_R1.fastq,” representing the forward reads. do fastp: This starts the for loop for fastp. -i ${r}: This inputs the files that end in “_R1.fastq” into fastp. -I ${r/R1/R2}: This line constructs the filename for the reverse reads by substituting “R1” with “R2” in the forward read variable. -o ${r/R1.fastq/processed_R1.fastq}: This line names the processed output forward reads by replacing the “R1.fastq” string from the forward read filename with “procressed_R1.fastq” -O ${r/R1.fastq/processed_R2.fastq}: This line names the processed output reverse reads by replacing the “R1.fastq” string from the forward read filename with “processed_R2.fastq” -h ${r/R1.fastq/fastp.html}: This line names the output html file by replacing the “R1.fastq” string from the forward read filename with “fastp.html” done: Ends the for loop.\nWhen you run this loop, it will automatically process each pair of forward and reverse reads in a single command. If we echo the above command, assuming amrfA1_R1.fastq and amrfA1_R2.fastq are the only files in our current directory, the output would be:\nfastp -i amrfA1_R1.fastq -I amrfA1_R2.fastq -o amrfA1_processed_R1.fastq -O amrfA1_processed_R2.fastq -h amrfA1_fastp.html\nOf course, just running the command above would get us the same result, but imagine doing this for hundreds of paired reads. For loops allow us to do this all in one command, instead of writing code for every set of paired reads.\n\n\n\nIn this section, you’ve learned how to use for loops with fastp to efficiently process paired-end sequencing data. Variable string substitution allows you to create dynamic filenames and automate the handling of both forward and reverse reads.\nExercise: Process Paired-End Data\n\nActivate your Conda environment.\nNavigate to the directory containing your paired-end sequencing data files.\nWrite a for loop like those provided above into your command line.\nRun the loop to process both forward and reverse reads.\nExamine the processed files, ensuring that the quality control and preprocessing were successful.\n\n\n\nExample code for exercise\n\nconda activate fastp_env\ncd sequencing_data\nfor r in *_R1.fastq; do fastp -i ${r} -I ${r/R1/R2} -o ${r/R1.fastq/processed_R1.fastq} -O ${r/R1.fastq/processed_R2.fastq} -h ${r/R1.fastq/fastp.html};done\n\nBy mastering the use of for loops and variable string substitution, you’re equipped to tackle paired-end data efficiently in your metagenomic analyses. There are other ways to manipulate variable strings in Bash, but substitution is by far the most used. If you want to read about other types of variable manipulation/expansion this wiki has a great table of various tricks:\nhttps://mywiki.wooledge.org/BashGuide/Parameters#Parameter_Expansion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AMRFlows Metagenomic Data Analysis Course",
    "section": "",
    "text": "This course will cover the bioinformatics workflow from the point of raw metagenomic sequencing data. You do not need prior bioinformatics experience to utilise this course.\nIn this course we’ll be covering the following topics:\n\nWhat is metagenomics?\nWhy do we want to do metagenomics?\nWhat is Unix/bash, and why do we use it?\nWhat software to use for bash command line?\nHow to navigate the command line interface.\nWhat is Conda and why do we use it to install tools?\nBasics of Illumina sequencing.\nCommon sequence file types.\nRead quality control.\nRead-based analyses.\n\nTools for taxonomic and functional composition.\n\nMetagenomic assembly and analyses.\n\nBinning and MAG generation\n\n\nDon’t worry if some/most of these concepts are foreign to you, as we’ll be going through each topic thoroughly. You’ll also be able to work through the content at your own pace. Throughout each topic, there will also be exercises to test if you’ve understood the content."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "AMRFlows Metagenomic Data Analysis Course",
    "section": "",
    "text": "This course will cover the bioinformatics workflow from the point of raw metagenomic sequencing data. You do not need prior bioinformatics experience to utilise this course.\nIn this course we’ll be covering the following topics:\n\nWhat is metagenomics?\nWhy do we want to do metagenomics?\nWhat is Unix/bash, and why do we use it?\nWhat software to use for bash command line?\nHow to navigate the command line interface.\nWhat is Conda and why do we use it to install tools?\nBasics of Illumina sequencing.\nCommon sequence file types.\nRead quality control.\nRead-based analyses.\n\nTools for taxonomic and functional composition.\n\nMetagenomic assembly and analyses.\n\nBinning and MAG generation\n\n\nDon’t worry if some/most of these concepts are foreign to you, as we’ll be going through each topic thoroughly. You’ll also be able to work through the content at your own pace. Throughout each topic, there will also be exercises to test if you’ve understood the content."
  },
  {
    "objectID": "14. Taxonomic classification and AMR gene detection of MAGs.html",
    "href": "14. Taxonomic classification and AMR gene detection of MAGs.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "The Genome Taxonomy Database Toolkit (GTDB-Tk) is a powerful software toolkit designed to provide objective taxonomic classifications for bacterial and archaeal genomes, particularly those obtained from metagenomic datasets. GTDB-Tk leverages the Genome Taxonomy Database (GTDB) to classify metagenome-assembled genomes (MAGs) efficiently and accurately. GTDB-Tk can be installed via conda through the Bioconda channel. Documentation for GTDB-Tk can be found here. Note that GTDB-Tk requires a large reference dataset of around 110G. Once you have everything installed, try and run the classify_wf on the MAGs we created with MetaBat2.\nAfter running GTDB-Tk, the following output files will be generated in the specified output directory:\n\nclassification_report.tsv: A tab-separated file containing the taxonomic classification of each genome, including the GTDB lineage.\nmarker_genes: Information about the identified marker genes, including their presence and the number of copies in each genome.\nalignment_files: Files containing the aligned sequences of the marker genes used for classification\ntree_files: Phylogenetic trees representing the placement of the classified genomes within the GTDB reference tree.\n\nThe classify.tree file is in a file format called Newick, which can be visualised via a variety of programs such as ITOL - an in-browser phylogeny viewer or simple to use tools such as FigTree or TreeViewer."
  },
  {
    "objectID": "14. Taxonomic classification and AMR gene detection of MAGs.html#taxonomic-classification-of-mags-with-gtdb-tk",
    "href": "14. Taxonomic classification and AMR gene detection of MAGs.html#taxonomic-classification-of-mags-with-gtdb-tk",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "The Genome Taxonomy Database Toolkit (GTDB-Tk) is a powerful software toolkit designed to provide objective taxonomic classifications for bacterial and archaeal genomes, particularly those obtained from metagenomic datasets. GTDB-Tk leverages the Genome Taxonomy Database (GTDB) to classify metagenome-assembled genomes (MAGs) efficiently and accurately. GTDB-Tk can be installed via conda through the Bioconda channel. Documentation for GTDB-Tk can be found here. Note that GTDB-Tk requires a large reference dataset of around 110G. Once you have everything installed, try and run the classify_wf on the MAGs we created with MetaBat2.\nAfter running GTDB-Tk, the following output files will be generated in the specified output directory:\n\nclassification_report.tsv: A tab-separated file containing the taxonomic classification of each genome, including the GTDB lineage.\nmarker_genes: Information about the identified marker genes, including their presence and the number of copies in each genome.\nalignment_files: Files containing the aligned sequences of the marker genes used for classification\ntree_files: Phylogenetic trees representing the placement of the classified genomes within the GTDB reference tree.\n\nThe classify.tree file is in a file format called Newick, which can be visualised via a variety of programs such as ITOL - an in-browser phylogeny viewer or simple to use tools such as FigTree or TreeViewer."
  },
  {
    "objectID": "14. Taxonomic classification and AMR gene detection of MAGs.html#antimicrobial-resistance-gene-detection-in-mags-with-amrfinderplus",
    "href": "14. Taxonomic classification and AMR gene detection of MAGs.html#antimicrobial-resistance-gene-detection-in-mags-with-amrfinderplus",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "13. Antimicrobial resistance gene detection in MAGs with AMRFinderPlus",
    "text": "13. Antimicrobial resistance gene detection in MAGs with AMRFinderPlus\nA common aim of metagenomic studies is to identify the presence of AMR genes and the context in which they are encoded. AMRFinderPlus is a powerful tool developed by the National Center for Biotechnology Information (NCBI) designed to identify antimicrobial resistance (AMR) genes, resistance-associated point mutations, as well certain other virulence factors. AMRFinderPlus utilizes a curated Reference Gene Database and a collection of Hidden Markov Models (HMMs) to accurately identify AMR genes and their associated functions. More details can be found on their github wiki. Install AMRFinderPlus with conda and run the tool on our MAGs FASTA file. The output of the tool is a tab separated file (.tsv). An example of this output can be found on the github wiki page. Important columns to look out for are the % Coverage of reference sequence and the % Identity to reference sequence. If these %s are notably low, they may suggest that the gene prediction is of low confidence.\nNote that AMRFinderPlus relies on a curated database that is regularly updated. It is possible that an AMR gene is present in your MAGs but not identified due to its absence from the database due to its novelty. Furthermore, the presence of an AMR gene within a genome is not a confirmation of its phenotypic resistance. Many AMR proteins may reduce an organisms suspectibility to an antimicrobial but not cross clinical breakpoints, or may provide no resistance at all due to mutations. Validation of detected AMR genes should be done through experimental methods."
  },
  {
    "objectID": "12. Assembly-based-analysis.html",
    "href": "12. Assembly-based-analysis.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Previously, we discussed the concept of assembly-based analysis of metagenomic data. In this section we will be beginning our assembly based journey by learning, well you guessed it, metagenomic assembly! This method is particularly beneficial for studying communities in complex environments, as it allows us to delve much deeper into the genetic and functional potential of the organisms within than read-based analysis.\nIn this section, I won’t be explictly giving you example code like in previous sections. Try and follow the github manuals for the tools and if you get stuck or confused, google is your friend. I believe in you!\n\n\nMEGAHIT is a popular de novo assembler widely used in metagenomic studies due to its efficiency in handling large datasets and complex metagenomes. MEGAHIT is available for download from the official GitHub repository: https://github.com/voutcn/megahit. Try and follow the instructions to install the tool via conda.\nTo run MEGAHIT, you will need to provide the input sequencing reads in FASTQ format. MEGAHIT supports both single-end and paired-end reads. Try and write a basic command for assembling your paired end reads. Leave any optional parameters as default. This will typically be the case for most datasets. Remember you can see the all of the flags for MEGAHIT by running megahit -h.\nNow that you’ve successfully run MEGAHIT, you’ll have some new files to look at within your specified output directory. The main output file of MEGAHIT is final.contigs.fa.\nLook inside final.contigs.fa.\nCan you tell the difference between this file and the reads you inputted?\n\n\n\nNow that we have our assembled contigs, it would be useful to get an idea of the quality of the assembly. One commonly used tool for this is QUAST, which stands for Quality Assessment Tool for Genome Assemblies. QUAST generates various metrics for evaluating the quality of an assembly and can be run with or without references. As we are looking at metagenomes from a complex environment, we will be running QUAST without any reference genomes. Installation and usage of QUAST is detailed on their sourceforge website: https://quast.sourceforge.net/index.html.\nWrite a command for running QUAST on final.contigs.fa with an output directory named megahit_quast-output. Keep other options as default for now.\nQUAST generates a lot of output files and a description of what each of them are can be found within the QUAST manual on their sourceforge page. For us, the most important is the report.txt file you can find within megahit_quast-output. The key metrics that are contained within this file are:\n\nTotal number of contigs is the total number of contigs in the assembly. A higher number of contigs may suggest a less contiguous assembly.\nLargest contig is the length of the longest contig in the assembly. Longer contigs are generally preferred as they indicate better assembly quality.\nTotal length is the total number of bases in the assembly.\nN50 is the length of the shortest contig in the set that contains the fewest (largest) contigs whose combined length represents at least 50% of the assembly.\nL50 is the number of contigs that make up the N50.\nGC Content is the percentage of guanine and cytosine in the assembly. This can provide insights into the composition and potential biases in the sequencing data.\n\nWhen evaluating the results of QUAST, there are a few specific red flags may indicate potential issues:\n\nHigh Number of Contigs: An excessive number of contigs relative to the expected complexity of the metagenome may suggest fragmentation. In metagenomic studies, a large number of contigs can be expected, but excessively high numbers could indicate poor assembly.\nLow N50 Value: A low N50 suggests that the assembly is highly fragmented, which can hinder the identification of complete genomes or operons, especially in diverse microbial communities.\nTotal Length Discrepancy: If the total length of the assembly is significantly shorter than expected based on the input data or the known diversity of the community, it may indicate missing sequences or poor assembly.\n\nOf course, whether these metrics are indicative of potential issues is completely dependent on the type of sample you sequenced and the nature of your experiment. It is therefore impossible to have concrete numbers for when these metrics represent issues, but they are important to keep in mind which is why we have covered them here."
  },
  {
    "objectID": "12. Assembly-based-analysis.html#metagenomic-assembly-with-megahit",
    "href": "12. Assembly-based-analysis.html#metagenomic-assembly-with-megahit",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Previously, we discussed the concept of assembly-based analysis of metagenomic data. In this section we will be beginning our assembly based journey by learning, well you guessed it, metagenomic assembly! This method is particularly beneficial for studying communities in complex environments, as it allows us to delve much deeper into the genetic and functional potential of the organisms within than read-based analysis.\nIn this section, I won’t be explictly giving you example code like in previous sections. Try and follow the github manuals for the tools and if you get stuck or confused, google is your friend. I believe in you!\n\n\nMEGAHIT is a popular de novo assembler widely used in metagenomic studies due to its efficiency in handling large datasets and complex metagenomes. MEGAHIT is available for download from the official GitHub repository: https://github.com/voutcn/megahit. Try and follow the instructions to install the tool via conda.\nTo run MEGAHIT, you will need to provide the input sequencing reads in FASTQ format. MEGAHIT supports both single-end and paired-end reads. Try and write a basic command for assembling your paired end reads. Leave any optional parameters as default. This will typically be the case for most datasets. Remember you can see the all of the flags for MEGAHIT by running megahit -h.\nNow that you’ve successfully run MEGAHIT, you’ll have some new files to look at within your specified output directory. The main output file of MEGAHIT is final.contigs.fa.\nLook inside final.contigs.fa.\nCan you tell the difference between this file and the reads you inputted?\n\n\n\nNow that we have our assembled contigs, it would be useful to get an idea of the quality of the assembly. One commonly used tool for this is QUAST, which stands for Quality Assessment Tool for Genome Assemblies. QUAST generates various metrics for evaluating the quality of an assembly and can be run with or without references. As we are looking at metagenomes from a complex environment, we will be running QUAST without any reference genomes. Installation and usage of QUAST is detailed on their sourceforge website: https://quast.sourceforge.net/index.html.\nWrite a command for running QUAST on final.contigs.fa with an output directory named megahit_quast-output. Keep other options as default for now.\nQUAST generates a lot of output files and a description of what each of them are can be found within the QUAST manual on their sourceforge page. For us, the most important is the report.txt file you can find within megahit_quast-output. The key metrics that are contained within this file are:\n\nTotal number of contigs is the total number of contigs in the assembly. A higher number of contigs may suggest a less contiguous assembly.\nLargest contig is the length of the longest contig in the assembly. Longer contigs are generally preferred as they indicate better assembly quality.\nTotal length is the total number of bases in the assembly.\nN50 is the length of the shortest contig in the set that contains the fewest (largest) contigs whose combined length represents at least 50% of the assembly.\nL50 is the number of contigs that make up the N50.\nGC Content is the percentage of guanine and cytosine in the assembly. This can provide insights into the composition and potential biases in the sequencing data.\n\nWhen evaluating the results of QUAST, there are a few specific red flags may indicate potential issues:\n\nHigh Number of Contigs: An excessive number of contigs relative to the expected complexity of the metagenome may suggest fragmentation. In metagenomic studies, a large number of contigs can be expected, but excessively high numbers could indicate poor assembly.\nLow N50 Value: A low N50 suggests that the assembly is highly fragmented, which can hinder the identification of complete genomes or operons, especially in diverse microbial communities.\nTotal Length Discrepancy: If the total length of the assembly is significantly shorter than expected based on the input data or the known diversity of the community, it may indicate missing sequences or poor assembly.\n\nOf course, whether these metrics are indicative of potential issues is completely dependent on the type of sample you sequenced and the nature of your experiment. It is therefore impossible to have concrete numbers for when these metrics represent issues, but they are important to keep in mind which is why we have covered them here."
  },
  {
    "objectID": "13. Genome binning.html",
    "href": "13. Genome binning.html",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Genome binning is a common step in metagenomic analysis and can be thought of an extension of genome assembly. In this process, we group assembled contigs into individual genome bins, in order to reconstruct metagenome-assembled genomes (MAGs). Genome binning tools use a variety of different approaches to group these contigs including: sequence composition-based binning, sequencing depth and coverage based binning, machine learning based methods, and hybrid methods that combine multiple informaton sources. Each of these methods have their own pros and cons, and the choice of which to use is often sample and experiment dependent. A detailed review on these binning approaches can be found here."
  },
  {
    "objectID": "13. Genome binning.html#genome-binning",
    "href": "13. Genome binning.html#genome-binning",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "",
    "text": "Genome binning is a common step in metagenomic analysis and can be thought of an extension of genome assembly. In this process, we group assembled contigs into individual genome bins, in order to reconstruct metagenome-assembled genomes (MAGs). Genome binning tools use a variety of different approaches to group these contigs including: sequence composition-based binning, sequencing depth and coverage based binning, machine learning based methods, and hybrid methods that combine multiple informaton sources. Each of these methods have their own pros and cons, and the choice of which to use is often sample and experiment dependent. A detailed review on these binning approaches can be found here."
  },
  {
    "objectID": "13. Genome binning.html#binning-with-metabat2",
    "href": "13. Genome binning.html#binning-with-metabat2",
    "title": "AMRflows Metagenomic Data Analysis Course",
    "section": "Binning with MetaBat2",
    "text": "Binning with MetaBat2\nIn this course we will be using MetaBat2 as our genome binning tool of choice. This tool is available through conda, but it is not recommended or supported by the developers of MetaBat. However, details on how to install the tool are quite clear on the tools bitbucket (Follow the non-Docker installation unless you are already familiar with Docker or want to experiment).\nMetaBat2 requires two input files: the assembled contigs we generated with MEGAHIT and a sorted BAM file containing the read coverage data for each contig. This can be generated by mapping the reads that we used to assemble the contigs to the assembled contigs, with a mapping tool such as BWA. Install BWA via conda and run:\nbwa mem final.contigs.fa sample1_R1.fq sample1_R2.fq &gt; sample1_aln.sam\nThis generates a SAM file, but MetaBat2 requires a sorted BAM file. A SAM is a human-readable alignment format and a BAM file is the binary version of that. We can convert a SAM file to a sorted BAM file with SAMtools, which can be installed via conda.\nsamtools sort sample1_aln.sam -o sample1_aln_sorted.bam\nNow that we have both our assembled contigs and a sorted bam file we can run MetaBat2 to get our MAGs. Look at the MetaBat2 bitbucket page and try and run MetaBat2 using our two input files and an output directory named sample1_metabat2_out, whilst keeping other options as default. MetaBat2 will then generate FASTA files for all the binned and unbinned contigs.\n\nEvaluating Quality of Metagenomic Bins with CheckM\nCheckM is a powerful tool designed to assess the quality of MAGs. It provides estimates of genome completeness and contamination by utilising lineage-specific marker genes that are ubiquitous and single-copy within a phylogenetic lineage. CheckM can be easily installed via conda. Reference data is required for CheckM to function and details on how to download and prepare the data can be found on CheckM’s wiki. The standard workflow for CheckM is the lineage_wf which can be run with:\ncheckm lineage_wf &lt;bin folder&gt; &lt;output folder&gt;\nCheckM provides several important functionalities for assessing genome quality including: * Estimation of Genome Completeness * Estimation of Genome Contamination * Identification of Potential Misassemblies\nYou should read the extensive CheckM wiki for details on how these metrics are calculated and what they mean. However, there are a few particular things to look out for in the CheckM output:\n\nLow Completeness Scores: Bins with completeness scores below 70% may indicate that significant portions of the genome are missing.\nHigh Contamination Scores: Contamination scores above 5% are concerning, as they suggest that the bin may contain DNA from multiple organisms, complicating downstream analyses.\nPresence of Duplicate Marker Genes: If marker genes that are expected to be single-copy appear as duplicates, it may indicate contamination or misassemblies.\nInconsistent Lineage Assignments: If a genome bin is classified with a broad marker set (e.g., domain-level) rather than a specific lineage, it may indicate uncertainty in the classification and quality of the genome.\n\nDecisions on what to do with bins are left up to the researcher but in my own pipelines I typically remove bins with less than 50% completeness and over 10% contamination. This ensures that the average quality of my bins are high, without discarding the majority of the bins."
  }
]